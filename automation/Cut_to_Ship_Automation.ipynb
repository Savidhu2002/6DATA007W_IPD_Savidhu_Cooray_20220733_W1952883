{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "487f0bd8-0803-4222-8066-50187e54f912",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: D:\\Savidhu_OneDrive\\OneDrive - Hirdaramani Group\\Projects\\Cut to Ship Prediction Model\\Cut to Ship Report Automation\\Cut_to_Ship_Week_43_Test.xlsx\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------\n",
    "# Cut to Ship Report Automation\n",
    "# Robust to column name variations\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "# -----------------------------\n",
    "# Helper functions for cleaning and standardizing inputs\n",
    "# -----------------------------\n",
    "def clean_cols(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Normalizing column names by removing extra whitespace and trimming edges.\"\"\"\n",
    "    df = df.copy()\n",
    "    df.columns = (\n",
    "        df.columns.astype(str)\n",
    "        .str.replace(r\"\\s+\", \" \", regex=True)  # collapsing whitespace/newlines/tabs\n",
    "        .str.strip()\n",
    "    )\n",
    "    return df\n",
    "\n",
    "def find_col(df: pd.DataFrame, text: str, prefer_exact: bool = True) -> str:\n",
    "    \"\"\"\n",
    "    Identifying the correct column even if the column name slightly varies.\n",
    "    - Trying exact match first (case-insensitive)\n",
    "    - Otherwise selecting the first column that contains the keyword\n",
    "    \"\"\"\n",
    "    cols = list(df.columns)\n",
    "\n",
    "    if prefer_exact:\n",
    "        for c in cols:\n",
    "            if c.strip().lower() == text.strip().lower():\n",
    "                return c\n",
    "\n",
    "    matches = [c for c in cols if text.lower() in c.lower()]\n",
    "    if not matches:\n",
    "        raise KeyError(\n",
    "            f\"Could not find a column containing '{text}'. Available columns:\\n{cols}\"\n",
    "        )\n",
    "    return matches[0]\n",
    "\n",
    "def ensure_datetime(s: pd.Series) -> pd.Series:\n",
    "    \"\"\"Converting values to datetime format safely (invalid values become NaT).\"\"\"\n",
    "    return pd.to_datetime(s, errors=\"coerce\")\n",
    "\n",
    "def safe_str(s: pd.Series) -> pd.Series:\n",
    "    \"\"\"Converting to string while keeping missing values safe for concatenations.\"\"\"\n",
    "    return s.fillna(\"\").astype(str)\n",
    "\n",
    "# -----------------------------\n",
    "# Defining file paths for the 7 source reports\n",
    "# -----------------------------\n",
    "D1_path = r\"D:\\Savidhu_OneDrive\\OneDrive - Hirdaramani Group\\Projects\\Cut to Ship Prediction Model\\Cut to Ship Report Automation\\Style Closure -Week 43.xlsx\"\n",
    "D2_path = r\"D:\\Savidhu_OneDrive\\OneDrive - Hirdaramani Group\\Projects\\Cut to Ship Prediction Model\\Cut to Ship Report Automation\\Garment Sales Order-Week 43.xlsx\"\n",
    "D3_path = r\"D:\\Savidhu_OneDrive\\OneDrive - Hirdaramani Group\\Projects\\Cut to Ship Prediction Model\\Cut to Ship Report Automation\\Order Book -Week 43.xlsx\"\n",
    "D4_path = r\"D:\\Savidhu_OneDrive\\OneDrive - Hirdaramani Group\\Projects\\Cut to Ship Prediction Model\\Cut to Ship Report Automation\\Transaction Summary -Week 43.xlsx\"\n",
    "D5_path = r\"D:\\Savidhu_OneDrive\\OneDrive - Hirdaramani Group\\Projects\\Cut to Ship Prediction Model\\Cut to Ship Report Automation\\Export Summary -Week 43.xlsx\"\n",
    "D6_path = r\"D:\\Savidhu_OneDrive\\OneDrive - Hirdaramani Group\\Projects\\Cut to Ship Prediction Model\\Cut to Ship Report Automation\\Last Shipment- Week 43.xlsx\"\n",
    "D7_path = r\"D:\\Savidhu_OneDrive\\OneDrive - Hirdaramani Group\\Projects\\Cut to Ship Prediction Model\\Cut to Ship Report Automation\\Master sheet - customer name.xlsx\"\n",
    "\n",
    "# -----------------------------\n",
    "# Loading all source datasets into pandas DataFrames\n",
    "# -----------------------------\n",
    "D1 = pd.read_excel(D1_path, sheet_name=\"Sheet1\")\n",
    "D2 = pd.read_excel(D2_path, sheet_name=\"Sheet1\")\n",
    "D3 = pd.read_excel(D3_path, sheet_name=\"Sheet1\")\n",
    "D4 = pd.read_excel(D4_path, sheet_name=\"Sheet1\")\n",
    "D5 = pd.read_excel(D5_path, sheet_name=\"Sheet1\")\n",
    "D6 = pd.read_excel(D6_path, sheet_name=\"Sheet1\")\n",
    "D7 = pd.read_excel(D7_path, sheet_name=\"Sheet1\")\n",
    "\n",
    "# Cleaning column headers to make matching reliable\n",
    "D1 = clean_cols(D1)\n",
    "D2 = clean_cols(D2)\n",
    "D3 = clean_cols(D3)\n",
    "D4 = clean_cols(D4)\n",
    "D5 = clean_cols(D5)\n",
    "D6 = clean_cols(D6)\n",
    "D7 = clean_cols(D7)\n",
    "\n",
    "# -----------------------------\n",
    "# Resolving the required column names dynamically (robust to naming variations)\n",
    "# -----------------------------\n",
    "# Resolving D2 filter columns\n",
    "D2_gen = find_col(D2, \"General sales order\")\n",
    "D2_sample = find_col(D2, \"Sample sales orders\")\n",
    "D2_salesman = find_col(D2, \"Salesman order\")\n",
    "D2_sales_order = find_col(D2, \"Sales order\")  # could be Sales order, Sales order.1, etc.\n",
    "\n",
    "# Resolving D1 columns\n",
    "D1_sales_order = find_col(D1, \"Sales order\")\n",
    "D1_customer = find_col(D1, \"Customer account\")\n",
    "D1_style_closed = find_col(D1, \"Style closed date\")\n",
    "\n",
    "# Resolving D7 columns\n",
    "D7_customer = find_col(D7, \"Customer\")\n",
    "D7_calling = find_col(D7, \"Calling Name\")\n",
    "\n",
    "# Resolving D3 columns\n",
    "D3_sales_order = find_col(D3, \"Sales order\")\n",
    "D3_division = find_col(D3, \"Division\")\n",
    "D3_season = find_col(D3, \"Season\")\n",
    "D3_style_no = find_col(D3, \"Style number\")\n",
    "D3_item_type = find_col(D3, \"Garment item type\")\n",
    "D3_site = find_col(D3, \"Site\")\n",
    "D3_set_garment = find_col(D3, \"Set garment\")\n",
    "D3_qty = find_col(D3, \"Quantity\")\n",
    "\n",
    "# Resolving D4 columns\n",
    "D4_sales_order = find_col(D4, \"Sales order\")\n",
    "D4_unit = find_col(D4, \"Unit\")\n",
    "D4_qty = find_col(D4, \"Qty\")\n",
    "\n",
    "# Resolving D5 columns\n",
    "D5_sales_order = find_col(D5, \"Sales order\")\n",
    "D5_site = find_col(D5, \"Site\")\n",
    "D5_invoice = find_col(D5, \"Customer invoice\")\n",
    "D5_date = find_col(D5, \"Date\")\n",
    "D5_invoice_qty = find_col(D5, \"Invoice qty\")\n",
    "D5_fob = find_col(D5, \"FOB\")\n",
    "\n",
    "# Resolving D6 columns\n",
    "D6_sales_order = find_col(D6, \"Sales order\")\n",
    "D6_approved_date = find_col(D6, \"Approved date\")\n",
    "\n",
    "# Converting date fields into datetime format\n",
    "D1[D1_style_closed] = ensure_datetime(D1[D1_style_closed])\n",
    "D6[D6_approved_date] = ensure_datetime(D6[D6_approved_date])\n",
    "D5[D5_date] = ensure_datetime(D5[D5_date])\n",
    "\n",
    "# -----------------------------\n",
    "# Filtering bulk sales orders from D2 (keeping only rows where all 3 flags are \"No\")\n",
    "# -----------------------------\n",
    "filtered_D2 = D2[\n",
    "    (D2[D2_gen] == \"No\") &\n",
    "    (D2[D2_sample] == \"No\") &\n",
    "    (D2[D2_salesman] == \"No\")\n",
    "][[D2_sales_order]].copy()\n",
    "\n",
    "# -----------------------------\n",
    "# Filtering D1 to keep only matching bulk sales orders (semi-join logic)\n",
    "# -----------------------------\n",
    "filtered_D1 = D1[D1[D1_sales_order].isin(filtered_D2[D2_sales_order])].copy()\n",
    "\n",
    "# -----------------------------\n",
    "# Initializing the result dataset using filtered Sales Orders\n",
    "# -----------------------------\n",
    "result = filtered_D1[[D1_sales_order]].copy()\n",
    "result = result.rename(columns={D1_sales_order: \"Sales_order\"})\n",
    "\n",
    "# -----------------------------\n",
    "# Adding Customer details from D1\n",
    "# -----------------------------\n",
    "result = result.merge(\n",
    "    D1[[D1_sales_order, D1_customer]].rename(columns={D1_sales_order: \"Sales_order\", D1_customer: \"Customer\"}),\n",
    "    on=\"Sales_order\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# Adding Calling Name from master mapping (D7) using Customer\n",
    "# -----------------------------\n",
    "result = result.merge(\n",
    "    D7[[D7_customer, D7_calling]].rename(columns={D7_customer: \"Customer\", D7_calling: \"Calling_Name\"}),\n",
    "    on=\"Customer\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# Adding order attributes from Order Book (D3) using Sales Order\n",
    "# -----------------------------\n",
    "result = result.merge(\n",
    "    D3[[D3_sales_order, D3_division, D3_season, D3_style_no, D3_item_type, D3_site, D3_set_garment]].rename(\n",
    "        columns={\n",
    "            D3_sales_order: \"Sales_order\",\n",
    "            D3_division: \"Division\",\n",
    "            D3_season: \"Season\",\n",
    "            D3_style_no: \"Style_number\",\n",
    "            D3_item_type: \"Garment_item_type\",\n",
    "            D3_site: \"Unit\",\n",
    "            D3_set_garment: \"Set_garment\",\n",
    "        }\n",
    "    ),\n",
    "    on=\"Sales_order\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# Adding \"Last Shipped\" date from D6 by taking the latest Approved date per Sales order\n",
    "# -----------------------------\n",
    "last_shipped = (\n",
    "    D6.groupby(D6_sales_order, as_index=False)[D6_approved_date]\n",
    "    .max()\n",
    "    .rename(columns={D6_sales_order: \"Sales_order\", D6_approved_date: \"Last_Shipped\"})\n",
    ")\n",
    "\n",
    "result = result.merge(last_shipped, on=\"Sales_order\", how=\"left\")\n",
    "\n",
    "# -----------------------------\n",
    "# Adding \"Style closed date\" from D1\n",
    "# -----------------------------\n",
    "result = result.merge(\n",
    "    D1[[D1_sales_order, D1_style_closed]].rename(columns={D1_sales_order: \"Sales_order\", D1_style_closed: \"Style_closed_date\"}),\n",
    "    on=\"Sales_order\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# Deriving Month and Week from Style closed date\n",
    "# -----------------------------\n",
    "result[\"Month\"] = result[\"Style_closed_date\"].dt.month_name().str[:3]\n",
    "\n",
    "iso_week = result[\"Style_closed_date\"].dt.isocalendar().week.astype(\"Int64\")\n",
    "result[\"Week\"] = iso_week.apply(lambda x: f\"Week {int(x):02d}\" if pd.notna(x) else np.nan)\n",
    "\n",
    "# -----------------------------\n",
    "# Deriving Operation type based on Sales order prefix\n",
    "# -----------------------------\n",
    "so_str = safe_str(result[\"Sales_order\"])\n",
    "result[\"Operation\"] = np.where(\n",
    "    so_str.str.startswith(\"N\"), \"Knit Operation\",\n",
    "    np.where(so_str.str.startswith(\"W\"), \"Woven Operation\", \"Other Operation\")\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# Removing excluded Customers from the dataset\n",
    "# -----------------------------\n",
    "result = result[~result[\"Customer\"].astype(str).str.lower().str.contains(\"oritapparels|southasiatextiles\", regex=True, na=False)].copy()\n",
    "\n",
    "# -----------------------------\n",
    "# Creating a unique \"Code\" key by combining Sales order and Unit\n",
    "# -----------------------------\n",
    "result[\"Code\"] = safe_str(result[\"Sales_order\"]) + safe_str(result[\"Unit\"])\n",
    "\n",
    "D3[\"Code\"] = safe_str(D3[D3_sales_order]) + safe_str(D3[D3_site])\n",
    "D4[\"Code\"] = (safe_str(D4[D4_sales_order]) + safe_str(D4[D4_unit])).str.upper()\n",
    "D5[\"Code\"] = safe_str(D5[D5_sales_order]) + safe_str(D5[D5_site])\n",
    "\n",
    "# -----------------------------\n",
    "# Aggregating Order Qty, Cut Qty, and Ship Qty by Code\n",
    "# -----------------------------\n",
    "order_qty = (\n",
    "    D3[~D3[D3_site].astype(str).str.contains(\"HIKH-SAMP|HKSAM\", regex=True, na=False)]\n",
    "    .groupby(\"Code\", as_index=False)[D3_qty]\n",
    "    .sum()\n",
    "    .rename(columns={D3_qty: \"Order_Qty\"})\n",
    ")\n",
    "\n",
    "cut_qty = (\n",
    "    D4.groupby(\"Code\", as_index=False)[D4_qty]\n",
    "    .sum()\n",
    "    .rename(columns={D4_qty: \"Cut_Qty\"})\n",
    ")\n",
    "\n",
    "ship_qty = (\n",
    "    D5[\n",
    "        ~D5[D5_invoice].astype(str).str.lower().str.match(r\"^(scl|rtn|dummy|sms|ss)\", na=False)\n",
    "    ]\n",
    "    .drop_duplicates(subset=[D5_invoice, D5_date, D5_invoice_qty])\n",
    "    .groupby(\"Code\", as_index=False)[D5_invoice_qty]\n",
    "    .sum()\n",
    "    .rename(columns={D5_invoice_qty: \"Ship_Qty\"})\n",
    ")\n",
    "\n",
    "# Merging aggregated quantities into the result dataset\n",
    "result = result.merge(order_qty, on=\"Code\", how=\"left\")\n",
    "result = result.merge(cut_qty, on=\"Code\", how=\"left\")\n",
    "result = result.merge(ship_qty, on=\"Code\", how=\"left\")\n",
    "\n",
    "# -----------------------------\n",
    "# Assigning Sales Order type\n",
    "# -----------------------------\n",
    "result[\"SO_Type\"] = \"Bulk\"\n",
    "\n",
    "# -----------------------------\n",
    "# Deriving Pcs and adjusting Cut Qty for pack styles\n",
    "# -----------------------------\n",
    "set_g = result[\"Set_garment\"].astype(str)\n",
    "style_num = result[\"Style_number\"].astype(str)\n",
    "\n",
    "pcs = np.where(\n",
    "    set_g.eq(\"Single\"), 1,\n",
    "    np.where(\n",
    "        set_g.str.contains(\"Pack\", na=False),\n",
    "        pd.to_numeric(style_num.str.extract(r\"(\\d+)(?=PK|P)\")[0], errors=\"coerce\"),\n",
    "        np.nan\n",
    "    )\n",
    ")\n",
    "\n",
    "result[\"Pcs\"] = pcs.astype(float)\n",
    "result.loc[result[\"Pcs\"] > 10, \"Pcs\"] = np.nan\n",
    "\n",
    "result[\"Cut_Qty\"] = pd.to_numeric(result[\"Cut_Qty\"], errors=\"coerce\")\n",
    "result.loc[result[\"Pcs\"].notna(), \"Cut_Qty\"] = result.loc[result[\"Pcs\"].notna(), \"Cut_Qty\"] / result.loc[result[\"Pcs\"].notna(), \"Pcs\"]\n",
    "\n",
    "# -----------------------------\n",
    "# Removing duplicate Codes (keeping the first record per Code)\n",
    "# -----------------------------\n",
    "result = result.drop_duplicates(subset=[\"Code\"], keep=\"first\").copy()\n",
    "\n",
    "# -----------------------------\n",
    "# Calculating Cut/Ship, Order/Ship, and Order/Cut ratios\n",
    "# -----------------------------\n",
    "result[\"Cut/Ship\"] = result[\"Ship_Qty\"] / result[\"Cut_Qty\"]\n",
    "result[\"Order/Ship\"] = result[\"Ship_Qty\"] / result[\"Order_Qty\"]\n",
    "result[\"Order/Cut\"] = result[\"Cut_Qty\"] / result[\"Order_Qty\"]\n",
    "\n",
    "# -----------------------------\n",
    "# Calculating FOB (sum of FOB * Invoice qty) by Code\n",
    "# -----------------------------\n",
    "D5_fob_total = pd.to_numeric(D5[D5_fob], errors=\"coerce\") * pd.to_numeric(D5[D5_invoice_qty], errors=\"coerce\")\n",
    "fob_data = (\n",
    "    D5.assign(FOB_Total=D5_fob_total)\n",
    "    .groupby(\"Code\", as_index=False)[\"FOB_Total\"]\n",
    "    .sum()\n",
    "    .rename(columns={\"FOB_Total\": \"FOB\"})\n",
    ")\n",
    "\n",
    "result = result.merge(fob_data, on=\"Code\", how=\"left\")\n",
    "\n",
    "# -----------------------------\n",
    "# Preparing the final output structure and column naming\n",
    "# -----------------------------\n",
    "result_out = result.rename(columns={\n",
    "    \"Sales_order\": \"Sales order\",\n",
    "    \"Calling_Name\": \"Calling Name\",\n",
    "    \"Division\": \"Div\",\n",
    "    \"Style_number\": \"Style number\",\n",
    "    \"Garment_item_type\": \"Garment item type\",\n",
    "    \"Last_Shipped\": \"Last Shipped\",\n",
    "    \"Style_closed_date\": \"Style closed date\",\n",
    "    \"Order_Qty\": \"Order Qty\",\n",
    "    \"Cut_Qty\": \"Cut Qty\",\n",
    "    \"Ship_Qty\": \"Ship Qty\",\n",
    "    \"SO_Type\": \"SO Type\",\n",
    "    \"Set_garment\": \"Set garment\",\n",
    "})\n",
    "\n",
    "# -----------------------------\n",
    "# Filtering output to only include Style closed date in 2024\n",
    "# -----------------------------\n",
    "result_out[\"Style closed date\"] = ensure_datetime(result_out[\"Style closed date\"])\n",
    "result_out = result_out[result_out[\"Style closed date\"].dt.year == 2024].copy()\n",
    "\n",
    "# -----------------------------\n",
    "# Filling missing Cut Qty values using Ship Qty where applicable\n",
    "# -----------------------------\n",
    "result_out[\"Cut Qty\"] = pd.to_numeric(result_out[\"Cut Qty\"], errors=\"coerce\")\n",
    "result_out[\"Ship Qty\"] = pd.to_numeric(result_out[\"Ship Qty\"], errors=\"coerce\")\n",
    "result_out.loc[result_out[\"Cut Qty\"].isna() & result_out[\"Ship Qty\"].notna(), \"Cut Qty\"] = result_out.loc[\n",
    "    result_out[\"Cut Qty\"].isna() & result_out[\"Ship Qty\"].notna(), \"Ship Qty\"\n",
    "]\n",
    "\n",
    "# -----------------------------\n",
    "# Excluding Corporate and Sample units from the final output\n",
    "# -----------------------------\n",
    "result_out[\"Unit\"] = result_out[\"Unit\"].astype(str)\n",
    "result_out = result_out[~result_out[\"Unit\"].str.contains(\"Corporate|Sample\", regex=True, na=False)].copy()\n",
    "\n",
    "# -----------------------------\n",
    "# Selecting and ordering final columns for export\n",
    "# -----------------------------\n",
    "final_cols = [\n",
    "    \"Sales order\",\n",
    "    \"Customer\",\n",
    "    \"Calling Name\",\n",
    "    \"Div\",\n",
    "    \"Season\",\n",
    "    \"Style number\",\n",
    "    \"Garment item type\",\n",
    "    \"Unit\",\n",
    "    \"Last Shipped\",\n",
    "    \"Style closed date\",\n",
    "    \"Month\",\n",
    "    \"Order Qty\",\n",
    "    \"Cut Qty\",\n",
    "    \"Ship Qty\",\n",
    "    \"Cut/Ship\",\n",
    "    \"Order/Ship\",\n",
    "    \"Order/Cut\",\n",
    "    \"SO Type\",\n",
    "    \"Week\",\n",
    "    \"Operation\",\n",
    "    \"Set garment\",\n",
    "    \"Pcs\",\n",
    "    \"Code\",\n",
    "    \"FOB\",\n",
    "]\n",
    "\n",
    "final_cols_existing = [c for c in final_cols if c in result_out.columns]\n",
    "result_out = result_out[final_cols_existing].copy()\n",
    "\n",
    "# -----------------------------\n",
    "# Writing the final Cut to Ship report output to Excel\n",
    "# -----------------------------\n",
    "output_path = (\n",
    "    r\"D:\\Savidhu_OneDrive\\OneDrive - Hirdaramani Group\\Projects\"\n",
    "    r\"\\Cut to Ship Prediction Model\\Cut to Ship Report Automation\"\n",
    "    r\"\\Cut_to_Ship_Week_43_Test.xlsx\"\n",
    ")\n",
    "\n",
    "result_out.to_excel(output_path, index=False)\n",
    "print(\"Saved:\", output_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29003b29-ca4b-41ab-8023-02808d559744",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
