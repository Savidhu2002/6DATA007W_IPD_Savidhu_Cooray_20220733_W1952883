{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "48549dd4-8dab-45cc-a79a-9ab502d56ccb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows ready for training: 13444\n",
      "\n",
      "STAGE A (Cut_Ratio) evaluation\n",
      "MAE: 0.023619205007307548\n",
      "R2 : 0.06624491296019752\n",
      "\n",
      "STAGE B (Loss_Ratio) evaluation\n",
      "MAE: 0.022096276532869007\n",
      "R2 : 0.5292730587231461\n",
      "\n",
      "Saved Stage A model: D:\\Savidhu_OneDrive\\OneDrive - Hirdaramani Group\\Projects\\Cut to Ship Prediction Model\\model_cut_ratio.pkl\n",
      "Saved Stage B model: D:\\Savidhu_OneDrive\\OneDrive - Hirdaramani Group\\Projects\\Cut to Ship Prediction Model\\model_loss_ratio.pkl\n",
      "Saved meta: D:\\Savidhu_OneDrive\\OneDrive - Hirdaramani Group\\Projects\\Cut to Ship Prediction Model\\model_meta.pkl\n",
      "\n",
      "Streamlit app written to: D:\\Savidhu_OneDrive\\OneDrive - Hirdaramani Group\\Projects\\Cut to Ship Prediction Model\\streamlit_app.py\n",
      "If it does not open automatically: http://localhost:8501\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import subprocess\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "\n",
    "# =========================\n",
    "# Configuration settings for data source, inputs, outputs, and file paths\n",
    "# =========================\n",
    "EXCEL_PATH = r\"D:\\Savidhu_OneDrive\\OneDrive - Hirdaramani Group\\Projects\\Cut to Ship Prediction Model\\Cut to Ship Report.xlsx\"\n",
    "SHEET_NAME = \"Final\"\n",
    "\n",
    "# Defining the final user input fields used in the Streamlit UI\n",
    "INPUT_COLS = [\n",
    "    \"Year\",\n",
    "    \"Calling Name\",\n",
    "    \"Div\",\n",
    "    \"Season\",\n",
    "    \"Garment item type\",\n",
    "    \"Unit\",\n",
    "    \"Operation\",\n",
    "    \"Month\",\n",
    "    \"Type\",\n",
    "    \"Operation 2\",\n",
    "    \"Pcs\",\n",
    "    \"Order Qty\",\n",
    "]\n",
    "\n",
    "# Defining the two target outputs that we want the pipeline to learn\n",
    "TARGET_CUT = \"Cut Qty\"\n",
    "TARGET_SHIP = \"Ship Qty\"\n",
    "\n",
    "# Setting up rare-category handling for high-cardinality categorical fields\n",
    "RARE_COLS = [\"Div\", \"Season\", \"Calling Name\", \"Garment item type\"]\n",
    "MIN_COUNT = 10\n",
    "OTHER_LABEL = \"OTHER\"\n",
    "\n",
    "# Defining reason columns (damage, rejection, transfer and adjustment-related quantities)\n",
    "# If any are missing in the file, the code will ignore them safely.\n",
    "REASON_COLS = [\n",
    "    \"Fabric Damage\",\n",
    "    \"Colour Shading\",\n",
    "    \"Finishing Damage\",\n",
    "    \"Shade Band\",\n",
    "    \"Pilot\",\n",
    "    \"Wash Reference Sample\",\n",
    "    \"Cut panel rejection qty\",\n",
    "    \"Sewing Reject Qty\",\n",
    "    \"EMB / Printing  Damages\",\n",
    "    \"Washing Damages\",\n",
    "    \"Sample qty\",\n",
    "    \"Shortage qty\",\n",
    "    \"Unreconciled qty -panel form\",\n",
    "    \"Unreconciled qty -GMT form\",\n",
    "    \"Second Quality\",\n",
    "    \"Good garments\",\n",
    "    \"PO Mix\",\n",
    "    \"Transfer to other SOD\",\n",
    "    \"Transfer from other SOD\",\n",
    "]\n",
    "\n",
    "# Defining where models, metadata, and Streamlit app code will be saved\n",
    "OUTPUT_DIR = os.path.dirname(EXCEL_PATH)\n",
    "MODEL_CUT_PATH = os.path.join(OUTPUT_DIR, \"model_cut_ratio.pkl\")\n",
    "MODEL_LOSS_PATH = os.path.join(OUTPUT_DIR, \"model_loss_ratio.pkl\")\n",
    "META_PATH = os.path.join(OUTPUT_DIR, \"model_meta.pkl\")\n",
    "APP_PATH = os.path.join(OUTPUT_DIR, \"streamlit_app.py\")\n",
    "\n",
    "# =========================\n",
    "# Helper functions for cleaning, bucketing, and safe feature creation\n",
    "# =========================\n",
    "def to_num(series):\n",
    "    # Converting text-based numbers (commas, %) into clean numeric values\n",
    "    s = (\n",
    "        series.astype(str)\n",
    "        .str.replace(\",\", \"\", regex=False)\n",
    "        .str.replace(\"%\", \"\", regex=False)\n",
    "        .str.strip()\n",
    "    )\n",
    "    return pd.to_numeric(s, errors=\"coerce\")\n",
    "\n",
    "def group_rare(df, col, min_count=10, other_label=\"OTHER\"):\n",
    "    # Grouping rare categories into a single \"OTHER\" bucket to reduce noise\n",
    "    vc = df[col].value_counts(dropna=False)\n",
    "    rare_vals = vc[vc < min_count].index\n",
    "    df[col] = df[col].replace(rare_vals, other_label)\n",
    "    return df\n",
    "\n",
    "def safe_sum(df, cols):\n",
    "    # Summing only the reason columns that actually exist in the dataset\n",
    "    present = [c for c in cols if c in df.columns]\n",
    "    if not present:\n",
    "        return pd.Series(0.0, index=df.index)\n",
    "    return df[present].sum(axis=1)\n",
    "\n",
    "# Building lookup tables to learn historical behavior, with fallback levels for missing granularity\n",
    "def build_lookup_tables(df, key_cols, feat_cols):\n",
    "    # Creating a full-granularity lookup table\n",
    "    full = df.groupby(key_cols, dropna=False)[feat_cols].mean().reset_index()\n",
    "\n",
    "    # Creating fallback lookup table without Calling Name (more general)\n",
    "    backoff1_keys = [c for c in key_cols if c != \"Calling Name\"]\n",
    "    backoff1 = df.groupby(backoff1_keys, dropna=False)[feat_cols].mean().reset_index()\n",
    "\n",
    "    # Creating fallback lookup table without Calling Name and Season (even more general)\n",
    "    backoff2_keys = [c for c in backoff1_keys if c != \"Season\"]\n",
    "    backoff2 = df.groupby(backoff2_keys, dropna=False)[feat_cols].mean().reset_index()\n",
    "\n",
    "    # Creating a global mean fallback (used when no match exists)\n",
    "    global_mean = df[feat_cols].mean().to_dict()\n",
    "\n",
    "    return {\n",
    "        \"key_cols\": key_cols,\n",
    "        \"backoff1_keys\": backoff1_keys,\n",
    "        \"backoff2_keys\": backoff2_keys,\n",
    "        \"full\": full,\n",
    "        \"backoff1\": backoff1,\n",
    "        \"backoff2\": backoff2,\n",
    "        \"global_mean\": global_mean,\n",
    "        \"feat_cols\": feat_cols\n",
    "    }\n",
    "\n",
    "def lookup_behavior(row_dict, lookups):\n",
    "    # Looking up historical behavior features using full and fallback matching\n",
    "    feat_cols = lookups[\"feat_cols\"]\n",
    "    global_mean = lookups[\"global_mean\"]\n",
    "\n",
    "    # Trying full matching first\n",
    "    full_keys = lookups[\"key_cols\"]\n",
    "    full_df = lookups[\"full\"]\n",
    "    mask = np.ones(len(full_df), dtype=bool)\n",
    "    for k in full_keys:\n",
    "        mask &= (full_df[k].astype(str).values == str(row_dict[k]))\n",
    "    match = full_df.loc[mask]\n",
    "    if len(match) > 0:\n",
    "        return {f: float(match.iloc[0][f]) for f in feat_cols}\n",
    "\n",
    "    # Trying fallback matching without Calling Name\n",
    "    b1_keys = lookups[\"backoff1_keys\"]\n",
    "    b1_df = lookups[\"backoff1\"]\n",
    "    mask = np.ones(len(b1_df), dtype=bool)\n",
    "    for k in b1_keys:\n",
    "        mask &= (b1_df[k].astype(str).values == str(row_dict[k]))\n",
    "    match = b1_df.loc[mask]\n",
    "    if len(match) > 0:\n",
    "        return {f: float(match.iloc[0][f]) for f in feat_cols}\n",
    "\n",
    "    # Trying fallback matching without Calling Name and Season\n",
    "    b2_keys = lookups[\"backoff2_keys\"]\n",
    "    b2_df = lookups[\"backoff2\"]\n",
    "    mask = np.ones(len(b2_df), dtype=bool)\n",
    "    for k in b2_keys:\n",
    "        mask &= (b2_df[k].astype(str).values == str(row_dict[k]))\n",
    "    match = b2_df.loc[mask]\n",
    "    if len(match) > 0:\n",
    "        return {f: float(match.iloc[0][f]) for f in feat_cols}\n",
    "\n",
    "    # Falling back to global mean when no match exists\n",
    "    return {f: float(global_mean.get(f, 0.0)) for f in feat_cols}\n",
    "\n",
    "# =========================\n",
    "# Loading data from Excel and validating required fields\n",
    "# =========================\n",
    "df = pd.read_excel(EXCEL_PATH, sheet_name=SHEET_NAME)\n",
    "df.columns = [str(c).strip() for c in df.columns]\n",
    "\n",
    "required = INPUT_COLS + [TARGET_CUT, TARGET_SHIP]\n",
    "missing = [c for c in required if c not in df.columns]\n",
    "if missing:\n",
    "    raise ValueError(f\"Missing columns in sheet '{SHEET_NAME}': {missing}\")\n",
    "\n",
    "# Keeping required fields + any available reason columns\n",
    "present_reason_cols = [c for c in REASON_COLS if c in df.columns]\n",
    "df = df[INPUT_COLS + [TARGET_CUT, TARGET_SHIP] + present_reason_cols].copy()\n",
    "\n",
    "# Converting numeric fields into proper numeric format\n",
    "df[\"Order Qty\"] = to_num(df[\"Order Qty\"])\n",
    "df[\"Pcs\"] = to_num(df[\"Pcs\"])\n",
    "df[TARGET_CUT] = to_num(df[TARGET_CUT])\n",
    "df[TARGET_SHIP] = to_num(df[TARGET_SHIP])\n",
    "\n",
    "for c in present_reason_cols:\n",
    "    df[c] = to_num(df[c]).fillna(0)\n",
    "\n",
    "# Cleaning invalid and extreme records to avoid training instability\n",
    "df = df.dropna(subset=[\"Order Qty\", \"Pcs\", TARGET_CUT, TARGET_SHIP])\n",
    "df = df[(df[\"Order Qty\"] > 0) & (df[\"Order Qty\"] < 1_000_000)]\n",
    "df = df[(df[\"Pcs\"] > 0) & (df[\"Pcs\"] < 1_000_000)]\n",
    "df = df[(df[TARGET_CUT] >= 0) & (df[TARGET_CUT] < 1_000_000)]\n",
    "df = df[(df[TARGET_SHIP] >= 0) & (df[TARGET_SHIP] < 1_000_000)]\n",
    "\n",
    "# Ensuring categorical fields are standardized as clean strings\n",
    "for c in INPUT_COLS:\n",
    "    if c not in [\"Order Qty\", \"Pcs\"]:\n",
    "        df[c] = df[c].astype(str).fillna(\"Unknown\").str.strip()\n",
    "\n",
    "# Bucketing rare categories into \"OTHER\" for selected categorical fields\n",
    "for c in RARE_COLS:\n",
    "    df = group_rare(df, c, min_count=MIN_COUNT, other_label=OTHER_LABEL)\n",
    "\n",
    "# =========================\n",
    "# Creating engineered targets for a 2-stage modeling approach\n",
    "# =========================\n",
    "eps = 1e-9\n",
    "df[\"Cut_Ratio\"] = df[TARGET_CUT] / (df[\"Order Qty\"] + eps)\n",
    "df[\"Loss_Ratio\"] = (df[TARGET_CUT] - df[TARGET_SHIP]) / (df[TARGET_CUT] + eps)\n",
    "\n",
    "# Applying sanity bounds to reduce noise and improve training stability\n",
    "df = df[(df[\"Cut_Ratio\"] > 0) & (df[\"Cut_Ratio\"] < 2)]\n",
    "df = df[(df[\"Loss_Ratio\"] >= 0) & (df[\"Loss_Ratio\"] < 1)]\n",
    "\n",
    "# =========================\n",
    "# Engineering historical behavior features using reason columns\n",
    "# =========================\n",
    "transfer_to = df[\"Transfer to other SOD\"] if \"Transfer to other SOD\" in df.columns else 0.0\n",
    "transfer_from = df[\"Transfer from other SOD\"] if \"Transfer from other SOD\" in df.columns else 0.0\n",
    "\n",
    "# Defining quality-related reason columns used for quality loss estimation\n",
    "quality_reason_candidates = [\n",
    "    \"Fabric Damage\",\n",
    "    \"Colour Shading\",\n",
    "    \"Finishing Damage\",\n",
    "    \"Shade Band\",\n",
    "    \"Pilot\",\n",
    "    \"Wash Reference Sample\",\n",
    "    \"Cut panel rejection qty\",\n",
    "    \"Sewing Reject Qty\",\n",
    "    \"EMB / Printing  Damages\",\n",
    "    \"Washing Damages\",\n",
    "    \"Sample qty\",\n",
    "    \"Shortage qty\",\n",
    "    \"Unreconciled qty -panel form\",\n",
    "    \"Unreconciled qty -GMT form\",\n",
    "    \"Second Quality\",\n",
    "    \"PO Mix\",\n",
    "]\n",
    "quality_cols = [c for c in quality_reason_candidates if c in df.columns]\n",
    "\n",
    "# Aggregating and converting reasons into ratios (relative to Cut Qty)\n",
    "df[\"Quality_Issue_Qty\"] = safe_sum(df, quality_cols)\n",
    "df[\"Net_Transfer_Qty\"] = (transfer_from - transfer_to) if isinstance(transfer_from, pd.Series) else 0.0\n",
    "\n",
    "df[\"Quality_Issue_Ratio\"] = df[\"Quality_Issue_Qty\"] / (df[TARGET_CUT] + eps)\n",
    "df[\"Net_Transfer_Ratio\"] = df[\"Net_Transfer_Qty\"] / (df[TARGET_CUT] + eps)\n",
    "\n",
    "# Clipping ratios to sensible limits\n",
    "df[\"Quality_Issue_Ratio\"] = df[\"Quality_Issue_Ratio\"].clip(0, 1)\n",
    "df[\"Net_Transfer_Ratio\"] = df[\"Net_Transfer_Ratio\"].clip(-1, 1)\n",
    "\n",
    "# =========================\n",
    "# Building historical lookup tables for expected behavior by similar orders\n",
    "# =========================\n",
    "KEY_COLS = [\n",
    "    \"Year\",\n",
    "    \"Calling Name\",\n",
    "    \"Div\",\n",
    "    \"Season\",\n",
    "    \"Garment item type\",\n",
    "    \"Unit\",\n",
    "    \"Operation\",\n",
    "    \"Month\",\n",
    "    \"Type\",\n",
    "    \"Operation 2\",\n",
    "]\n",
    "\n",
    "BEHAVIOR_FEATURES = [\"Quality_Issue_Ratio\", \"Net_Transfer_Ratio\"]\n",
    "\n",
    "lookups = build_lookup_tables(df, KEY_COLS, BEHAVIOR_FEATURES)\n",
    "\n",
    "# Adding historical behavior features row-by-row using lookup matching\n",
    "def add_behavior_features(df, lookups):\n",
    "    out_quality = []\n",
    "    out_transfer = []\n",
    "    for _, r in df.iterrows():\n",
    "        row_dict = {k: r[k] for k in lookups[\"key_cols\"]}\n",
    "        feats = lookup_behavior(row_dict, lookups)\n",
    "        out_quality.append(feats[\"Quality_Issue_Ratio\"])\n",
    "        out_transfer.append(feats[\"Net_Transfer_Ratio\"])\n",
    "    df[\"Hist_Quality_Issue_Ratio\"] = out_quality\n",
    "    df[\"Hist_Net_Transfer_Ratio\"] = out_transfer\n",
    "    return df\n",
    "\n",
    "df = add_behavior_features(df, lookups)\n",
    "\n",
    "print(\"Rows ready for training:\", len(df))\n",
    "\n",
    "# =========================\n",
    "# Training Stage A model: predicting Cut_Ratio\n",
    "# =========================\n",
    "X_base = df[INPUT_COLS].copy()\n",
    "X_base[\"Hist_Quality_Issue_Ratio\"] = df[\"Hist_Quality_Issue_Ratio\"].astype(float)\n",
    "X_base[\"Hist_Net_Transfer_Ratio\"] = df[\"Hist_Net_Transfer_Ratio\"].astype(float)\n",
    "\n",
    "y_cut = df[\"Cut_Ratio\"].astype(float)\n",
    "\n",
    "cat_cols = [c for c in INPUT_COLS if c not in [\"Order Qty\", \"Pcs\"]]\n",
    "num_cols = [\"Order Qty\", \"Pcs\", \"Hist_Quality_Issue_Ratio\", \"Hist_Net_Transfer_Ratio\"]\n",
    "\n",
    "# Building preprocessing pipeline (OneHot for categoricals + pass-through numeric fields)\n",
    "preprocess = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), cat_cols),\n",
    "        (\"num\", \"passthrough\", num_cols),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Building the Stage A Random Forest model pipeline\n",
    "cut_model = Pipeline(steps=[\n",
    "    (\"prep\", preprocess),\n",
    "    (\"model\", RandomForestRegressor(\n",
    "        n_estimators=600,\n",
    "        max_depth=22,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    ))\n",
    "])\n",
    "\n",
    "# Splitting data and training the Stage A model\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_base, y_cut, test_size=0.2, random_state=42)\n",
    "cut_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluating Stage A predictions\n",
    "pred_cut = cut_model.predict(X_test)\n",
    "print(\"\\nSTAGE A (Cut_Ratio) evaluation\")\n",
    "print(\"MAE:\", mean_absolute_error(y_test, pred_cut))\n",
    "print(\"R2 :\", r2_score(y_test, pred_cut))\n",
    "\n",
    "# =========================\n",
    "# Training Stage B model: predicting Loss_Ratio (uses Cut_Ratio as an input feature)\n",
    "# =========================\n",
    "X_loss = X_base.copy()\n",
    "X_loss[\"Cut_Ratio\"] = df[\"Cut_Ratio\"].astype(float)\n",
    "y_loss = df[\"Loss_Ratio\"].astype(float)\n",
    "\n",
    "num_cols_loss = num_cols + [\"Cut_Ratio\"]\n",
    "\n",
    "# Building preprocessing pipeline for Stage B (includes Cut_Ratio as numeric input)\n",
    "preprocess_loss = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), cat_cols),\n",
    "        (\"num\", \"passthrough\", num_cols_loss),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Building the Stage B Random Forest model pipeline\n",
    "loss_model = Pipeline(steps=[\n",
    "    (\"prep\", preprocess_loss),\n",
    "    (\"model\", RandomForestRegressor(\n",
    "        n_estimators=600,\n",
    "        max_depth=22,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    ))\n",
    "])\n",
    "\n",
    "# Splitting data and training the Stage B model\n",
    "X_train2, X_test2, y_train2, y_test2 = train_test_split(X_loss, y_loss, test_size=0.2, random_state=42)\n",
    "loss_model.fit(X_train2, y_train2)\n",
    "\n",
    "# Evaluating Stage B predictions\n",
    "pred_loss = loss_model.predict(X_test2)\n",
    "print(\"\\nSTAGE B (Loss_Ratio) evaluation\")\n",
    "print(\"MAE:\", mean_absolute_error(y_test2, pred_loss))\n",
    "print(\"R2 :\", r2_score(y_test2, pred_loss))\n",
    "\n",
    "# =========================\n",
    "# Saving trained models and metadata (including lookups and allowed dropdown values)\n",
    "# =========================\n",
    "joblib.dump(cut_model, MODEL_CUT_PATH)\n",
    "joblib.dump(loss_model, MODEL_LOSS_PATH)\n",
    "\n",
    "allowed_values = {}\n",
    "for c in INPUT_COLS:\n",
    "    if c not in [\"Order Qty\", \"Pcs\"]:\n",
    "        allowed_values[c] = sorted(df[c].astype(str).unique().tolist())\n",
    "\n",
    "meta = {\n",
    "    \"excel_path\": EXCEL_PATH,\n",
    "    \"sheet_name\": SHEET_NAME,\n",
    "    \"input_cols\": INPUT_COLS,\n",
    "    \"key_cols\": KEY_COLS,\n",
    "    \"rare_cols\": RARE_COLS,\n",
    "    \"min_count\": MIN_COUNT,\n",
    "    \"other_label\": OTHER_LABEL,\n",
    "    \"allowed_values\": allowed_values,\n",
    "    \"behavior_features\": BEHAVIOR_FEATURES,\n",
    "    \"lookups\": {\n",
    "        \"key_cols\": lookups[\"key_cols\"],\n",
    "        \"backoff1_keys\": lookups[\"backoff1_keys\"],\n",
    "        \"backoff2_keys\": lookups[\"backoff2_keys\"],\n",
    "        \"full\": lookups[\"full\"],\n",
    "        \"backoff1\": lookups[\"backoff1\"],\n",
    "        \"backoff2\": lookups[\"backoff2\"],\n",
    "        \"global_mean\": lookups[\"global_mean\"],\n",
    "        \"feat_cols\": lookups[\"feat_cols\"]\n",
    "    }\n",
    "}\n",
    "joblib.dump(meta, META_PATH)\n",
    "\n",
    "print(\"\\nSaved Stage A model:\", MODEL_CUT_PATH)\n",
    "print(\"Saved Stage B model:\", MODEL_LOSS_PATH)\n",
    "print(\"Saved meta:\", META_PATH)\n",
    "\n",
    "# =========================\n",
    "# Writing the Streamlit app code (2-stage prediction, no reason inputs required)\n",
    "# =========================\n",
    "app_code = f\"\"\"\n",
    "import streamlit as st\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "\n",
    "MODEL_CUT_PATH = r\"{MODEL_CUT_PATH}\"\n",
    "MODEL_LOSS_PATH = r\"{MODEL_LOSS_PATH}\"\n",
    "META_PATH = r\"{META_PATH}\"\n",
    "\n",
    "cut_model = joblib.load(MODEL_CUT_PATH)\n",
    "loss_model = joblib.load(MODEL_LOSS_PATH)\n",
    "meta = joblib.load(META_PATH)\n",
    "\n",
    "INPUT_COLS = meta[\"input_cols\"]\n",
    "ALLOWED = meta[\"allowed_values\"]\n",
    "RARE_COLS = set(meta[\"rare_cols\"])\n",
    "OTHER_LABEL = meta[\"other_label\"]\n",
    "\n",
    "lookups = meta[\"lookups\"]\n",
    "feat_cols = lookups[\"feat_cols\"]\n",
    "\n",
    "full_df = pd.DataFrame(lookups[\"full\"])\n",
    "b1_df = pd.DataFrame(lookups[\"backoff1\"])\n",
    "b2_df = pd.DataFrame(lookups[\"backoff2\"])\n",
    "global_mean = lookups[\"global_mean\"]\n",
    "\n",
    "def lookup_behavior(row_dict):\n",
    "    # Trying full matching first\n",
    "    mask = np.ones(len(full_df), dtype=bool)\n",
    "    for k in lookups[\"key_cols\"]:\n",
    "        mask &= (full_df[k].astype(str).values == str(row_dict[k]))\n",
    "    m = full_df.loc[mask]\n",
    "    if len(m) > 0:\n",
    "        return {{f: float(m.iloc[0][f]) for f in feat_cols}}\n",
    "\n",
    "    # Trying fallback matching without Calling Name\n",
    "    mask = np.ones(len(b1_df), dtype=bool)\n",
    "    for k in lookups[\"backoff1_keys\"]:\n",
    "        mask &= (b1_df[k].astype(str).values == str(row_dict[k]))\n",
    "    m = b1_df.loc[mask]\n",
    "    if len(m) > 0:\n",
    "        return {{f: float(m.iloc[0][f]) for f in feat_cols}}\n",
    "\n",
    "    # Trying fallback matching without Calling Name and Season\n",
    "    mask = np.ones(len(b2_df), dtype=bool)\n",
    "    for k in lookups[\"backoff2_keys\"]:\n",
    "        mask &= (b2_df[k].astype(str).values == str(row_dict[k]))\n",
    "    m = b2_df.loc[mask]\n",
    "    if len(m) > 0:\n",
    "        return {{f: float(m.iloc[0][f]) for f in feat_cols}}\n",
    "\n",
    "    # Falling back to global mean when no match exists\n",
    "    return {{f: float(global_mean.get(f, 0.0)) for f in feat_cols}}\n",
    "\n",
    "st.set_page_config(page_title=\"Cut to Ship Prediction\", layout=\"wide\")\n",
    "st.title(\"Cut to Ship Prediction (2-Stage Model)\")\n",
    "\n",
    "st.write(\n",
    "    \"Stage A predicts the cutting plan (Cut Qty based on Cut Ratio). \"\n",
    "    \"Stage B predicts execution loss (Ship Qty based on Loss Ratio). \"\n",
    "    \"The reasons data is used as historical behavior patterns, so users do not need to enter it.\"\n",
    ")\n",
    "\n",
    "st.subheader(\"Enter Order Details\")\n",
    "cols = st.columns(3)\n",
    "inputs = {{}}\n",
    "\n",
    "for i, col in enumerate(INPUT_COLS):\n",
    "    with cols[i % 3]:\n",
    "        if col in [\"Order Qty\", \"Pcs\"]:\n",
    "            inputs[col] = st.number_input(col, min_value=1, step=1)\n",
    "        else:\n",
    "            options = ALLOWED.get(col, [])\n",
    "            if options:\n",
    "                inputs[col] = st.selectbox(col, options)\n",
    "            else:\n",
    "                inputs[col] = st.text_input(col, value=\"Unknown\")\n",
    "\n",
    "# Ensuring categorical values are clean strings\n",
    "for c in INPUT_COLS:\n",
    "    if c not in [\"Order Qty\", \"Pcs\"]:\n",
    "        inputs[c] = str(inputs[c]).strip()\n",
    "\n",
    "# Applying rare bucketing at input time\n",
    "for c in RARE_COLS:\n",
    "    if inputs.get(c, OTHER_LABEL) not in ALLOWED.get(c, []):\n",
    "        inputs[c] = OTHER_LABEL\n",
    "\n",
    "# Looking up historical behavior features for the selected input combination\n",
    "row_dict = {{k: inputs[k] for k in lookups[\"key_cols\"]}}\n",
    "beh = lookup_behavior(row_dict)\n",
    "\n",
    "X = pd.DataFrame([[inputs[c] for c in INPUT_COLS]], columns=INPUT_COLS)\n",
    "X[\"Hist_Quality_Issue_Ratio\"] = beh.get(\"Quality_Issue_Ratio\", 0.0)\n",
    "X[\"Hist_Net_Transfer_Ratio\"] = beh.get(\"Net_Transfer_Ratio\", 0.0)\n",
    "\n",
    "if st.button(\"Predict\"):\n",
    "    order_qty = float(inputs[\"Order Qty\"])\n",
    "    eps = 1e-9\n",
    "\n",
    "    # Stage A prediction\n",
    "    cut_ratio_pred = float(cut_model.predict(X)[0])\n",
    "    cut_ratio_pred = max(0.0, min(2.0, cut_ratio_pred))\n",
    "    cut_qty_pred = order_qty * cut_ratio_pred\n",
    "\n",
    "    # Stage B prediction (using predicted cut ratio)\n",
    "    X2 = X.copy()\n",
    "    X2[\"Cut_Ratio\"] = cut_ratio_pred\n",
    "    loss_ratio_pred = float(loss_model.predict(X2)[0])\n",
    "    loss_ratio_pred = max(0.0, min(1.0, loss_ratio_pred))\n",
    "\n",
    "    ship_qty_pred = cut_qty_pred * (1 - loss_ratio_pred)\n",
    "\n",
    "    # Calculating ratios for interpretability\n",
    "    cut_ship = ship_qty_pred / (cut_qty_pred + eps)\n",
    "    order_ship = ship_qty_pred / (order_qty + eps)\n",
    "    order_cut = cut_qty_pred / (order_qty + eps)\n",
    "\n",
    "    # Assigning a simple risk category based on ratio thresholds\n",
    "    if cut_ship < 0.95 or order_ship < 0.95:\n",
    "        risk = \"HIGH RISK\"\n",
    "        color = \"red\"\n",
    "    elif cut_ship < 0.98 or order_ship < 0.98:\n",
    "        risk = \"MEDIUM RISK\"\n",
    "        color = \"orange\"\n",
    "    else:\n",
    "        risk = \"LOW RISK\"\n",
    "        color = \"green\"\n",
    "\n",
    "    st.subheader(\"Prediction Results\")\n",
    "\n",
    "    c1, c2, c3 = st.columns(3)\n",
    "    c1.metric(\"Predicted Cut Qty\", f\"{{int(round(cut_qty_pred)):,}}\")\n",
    "    c2.metric(\"Predicted Ship Qty\", f\"{{int(round(ship_qty_pred)):,}}\")\n",
    "    c3.metric(\"Order Qty\", f\"{{int(order_qty):,}}\")\n",
    "\n",
    "    c4, c5, c6 = st.columns(3)\n",
    "    c4.metric(\"Cut / Ship\", round(float(cut_ship), 3))\n",
    "    c5.metric(\"Order / Ship\", round(float(order_ship), 3))\n",
    "    c6.metric(\"Order / Cut\", round(float(order_cut), 3))\n",
    "\n",
    "    st.markdown(f\"<h3 style='color:{{color}}'>Overall Risk: {{risk}}</h3>\", unsafe_allow_html=True)\n",
    "\n",
    "    with st.expander(\"Model details used (historical behavior features)\"):\n",
    "        st.write({{\n",
    "            \"Hist_Quality_Issue_Ratio\": float(X[\"Hist_Quality_Issue_Ratio\"].iloc[0]),\n",
    "            \"Hist_Net_Transfer_Ratio\": float(X[\"Hist_Net_Transfer_Ratio\"].iloc[0]),\n",
    "            \"Pred_Cut_Ratio\": float(cut_ratio_pred),\n",
    "            \"Pred_Loss_Ratio\": float(loss_ratio_pred),\n",
    "        }})\n",
    "\"\"\"\n",
    "\n",
    "with open(APP_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(app_code)\n",
    "\n",
    "print(\"\\nStreamlit app written to:\", APP_PATH)\n",
    "\n",
    "# =========================\n",
    "# Launching the Streamlit app directly from the notebook\n",
    "# =========================\n",
    "time.sleep(1)\n",
    "subprocess.Popen([sys.executable, \"-m\", \"streamlit\", \"run\", APP_PATH], cwd=OUTPUT_DIR)\n",
    "print(\"If it does not open automatically: http://localhost:8501\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "38da7524-3dc1-42be-b621-80fa36e49441",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL PERFORMANCE (2026 UNSEEN DATA)\n",
      "Rows: 112\n",
      "\n",
      "CUT QTY\n",
      "MAE: 68.78\n",
      "MAPE: 1.39%\n",
      "Accuracy (100 - MAPE): 98.61%\n",
      "R²: 0.9993\n",
      "Within ±5%: 98.21%\n",
      "Within ±10%: 99.11%\n",
      "\n",
      "SHIP QTY\n",
      "MAE: 173.30\n",
      "MAPE: 2.92%\n",
      "Accuracy (100 - MAPE): 97.08%\n",
      "R²: 0.9973\n",
      "Within ±5%: 87.50%\n",
      "Within ±10%: 99.11%\n",
      "\n",
      "Sample comparison (first 10 rows):\n",
      "   Order Qty  Cut Qty  Ship Qty  Pred_CutQty  Pred_ShipQty  Cut_Error_%  \\\n",
      "0       2404   2427.0      2298       2435.0        2337.0     0.329625   \n",
      "1      25788  26098.0     26002      26214.0       25026.0     0.444479   \n",
      "2       4646   4801.0      4699       4761.0        4593.0     0.833160   \n",
      "3      11479  11822.0     11668      12196.0       11484.0     3.163593   \n",
      "4        889    920.0       897        903.0         871.0     1.847826   \n",
      "5       2719   2850.0      2829       2784.0        2648.0     2.315789   \n",
      "6       4797   5009.0      4994       4916.0        4731.0     1.856658   \n",
      "7       4751   4983.0      4974       4869.0        4689.0     2.287778   \n",
      "8       4740   4902.0      4958       4849.0        4650.0     1.081191   \n",
      "9       2795   2857.0      2782       2833.0        2750.0     0.840042   \n",
      "\n",
      "   Ship_Error_%  \n",
      "0      1.697128  \n",
      "1      3.753557  \n",
      "2      2.255799  \n",
      "3      1.576963  \n",
      "4      2.898551  \n",
      "5      6.398021  \n",
      "6      5.266320  \n",
      "7      5.729795  \n",
      "8      6.212182  \n",
      "9      1.150252  \n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------\n",
    "# Cut to Ship Model Evaluation (2026) - Using industry standard metrics\n",
    "# Metrics: MAE, MAPE, Accuracy (100 - MAPE), R², Within ±5%, Within ±10%\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "\n",
    "# =========================\n",
    "# Configuration settings for evaluation data + saved model file locations\n",
    "# =========================\n",
    "EXCEL_PATH = r\"D:\\Savidhu_OneDrive\\OneDrive - Hirdaramani Group\\Projects\\Cut to Ship Prediction Model\\Cut to Ship Report.xlsx\"\n",
    "SHEET_NAME = \"2026\"\n",
    "\n",
    "OUTPUT_DIR = os.path.dirname(EXCEL_PATH)\n",
    "MODEL_CUT_PATH = os.path.join(OUTPUT_DIR, \"model_cut_ratio.pkl\")\n",
    "MODEL_LOSS_PATH = os.path.join(OUTPUT_DIR, \"model_loss_ratio.pkl\")\n",
    "META_PATH = os.path.join(OUTPUT_DIR, \"model_meta.pkl\")\n",
    "\n",
    "# Defining the actual values we will compare predictions against\n",
    "TARGET_CUT = \"Cut Qty\"\n",
    "TARGET_SHIP = \"Ship Qty\"\n",
    "\n",
    "# =========================\n",
    "# Helper functions for numeric cleaning and evaluation metrics\n",
    "# =========================\n",
    "def to_num(series: pd.Series) -> pd.Series:\n",
    "    # Converting values into numeric format (removing commas, %, and handling blanks)\n",
    "    s = (\n",
    "        series.astype(str)\n",
    "        .str.replace(\",\", \"\", regex=False)\n",
    "        .str.replace(\"%\", \"\", regex=False)\n",
    "        .str.strip()\n",
    "    )\n",
    "    s = s.replace({\"()\": \"0\", \"\": np.nan, \"nan\": np.nan, \"None\": np.nan})\n",
    "    return pd.to_numeric(s, errors=\"coerce\")\n",
    "\n",
    "def mape_pct(y_true, y_pred) -> float:\n",
    "    # Calculating Mean Absolute Percentage Error (MAPE) as a percentage\n",
    "    eps = 1e-9\n",
    "    y_true = np.asarray(y_true, dtype=float)\n",
    "    y_pred = np.asarray(y_pred, dtype=float)\n",
    "    return float(np.mean(np.abs((y_true - y_pred) / np.maximum(np.abs(y_true), eps))) * 100)\n",
    "\n",
    "def accuracy_from_mape(y_true, y_pred) -> float:\n",
    "    # Converting MAPE into an intuitive accuracy score (100 - MAPE)\n",
    "    return float(np.clip(100.0 - mape_pct(y_true, y_pred), 0.0, 100.0))\n",
    "\n",
    "def within_pct(y_true, y_pred, pct: float) -> float:\n",
    "    # Calculating the % of rows where prediction error is within a given threshold\n",
    "    eps = 1e-9\n",
    "    y_true = np.asarray(y_true, dtype=float)\n",
    "    y_pred = np.asarray(y_pred, dtype=float)\n",
    "    return float(\n",
    "        np.mean(np.abs(y_true - y_pred) / np.maximum(np.abs(y_true), eps) <= pct / 100.0) * 100\n",
    "    )\n",
    "\n",
    "def print_metrics(title: str, y_true, y_pred):\n",
    "    # Printing the full evaluation summary for a given target\n",
    "    y_true = np.asarray(y_true, dtype=float)\n",
    "    y_pred = np.asarray(y_pred, dtype=float)\n",
    "\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    mape = mape_pct(y_true, y_pred)\n",
    "    acc = accuracy_from_mape(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    w5 = within_pct(y_true, y_pred, 5)\n",
    "    w10 = within_pct(y_true, y_pred, 10)\n",
    "\n",
    "    print(f\"\\n{title}\")\n",
    "    print(f\"MAE: {mae:.2f}\")\n",
    "    print(f\"MAPE: {mape:.2f}%\")\n",
    "    print(f\"Accuracy (100 - MAPE): {acc:.2f}%\")\n",
    "    print(f\"R²: {r2:.4f}\")\n",
    "    print(f\"Within ±5%: {w5:.2f}%\")\n",
    "    print(f\"Within ±10%: {w10:.2f}%\")\n",
    "\n",
    "# =========================\n",
    "# Loading saved models and metadata required for feature building\n",
    "# =========================\n",
    "cut_model = joblib.load(MODEL_CUT_PATH)\n",
    "loss_model = joblib.load(MODEL_LOSS_PATH)\n",
    "meta = joblib.load(META_PATH)\n",
    "\n",
    "INPUT_COLS = meta[\"input_cols\"]\n",
    "RARE_COLS = meta[\"rare_cols\"]\n",
    "OTHER_LABEL = meta[\"other_label\"]\n",
    "lookups = meta[\"lookups\"]\n",
    "\n",
    "# Preparing lookup tables used to generate historical behavior features\n",
    "full_df = pd.DataFrame(lookups[\"full\"])\n",
    "b1_df = pd.DataFrame(lookups[\"backoff1\"])\n",
    "b2_df = pd.DataFrame(lookups[\"backoff2\"])\n",
    "global_mean = lookups[\"global_mean\"]\n",
    "feat_cols = lookups[\"feat_cols\"]\n",
    "\n",
    "def lookup_behavior(row_dict):\n",
    "    # Looking up historical behavior features using full matching first, then fallbacks\n",
    "    for df_, keys in [\n",
    "        (full_df, lookups[\"key_cols\"]),\n",
    "        (b1_df, lookups[\"backoff1_keys\"]),\n",
    "        (b2_df, lookups[\"backoff2_keys\"]),\n",
    "    ]:\n",
    "        mask = np.ones(len(df_), dtype=bool)\n",
    "        for k in keys:\n",
    "            mask &= df_[k].astype(str).values == str(row_dict[k])\n",
    "        m = df_.loc[mask]\n",
    "        if len(m) > 0:\n",
    "            return {f: float(m.iloc[0][f]) for f in feat_cols}\n",
    "    return {f: float(global_mean.get(f, 0.0)) for f in feat_cols}\n",
    "\n",
    "# =========================\n",
    "# Loading unseen 2026 data and validating the required structure\n",
    "# =========================\n",
    "df = pd.read_excel(EXCEL_PATH, sheet_name=SHEET_NAME)\n",
    "df.columns = [str(c).strip() for c in df.columns]\n",
    "\n",
    "required = INPUT_COLS + [TARGET_CUT, TARGET_SHIP]\n",
    "missing = [c for c in required if c not in df.columns]\n",
    "if missing:\n",
    "    raise ValueError(f\"Missing columns in 2026 sheet: {missing}\")\n",
    "\n",
    "# Keeping only the required evaluation fields\n",
    "df = df[INPUT_COLS + [TARGET_CUT, TARGET_SHIP]].copy()\n",
    "\n",
    "# Cleaning numeric columns needed for evaluation\n",
    "df[\"Order Qty\"] = to_num(df[\"Order Qty\"])\n",
    "df[\"Pcs\"] = to_num(df[\"Pcs\"])\n",
    "df[TARGET_CUT] = to_num(df[TARGET_CUT])\n",
    "df[TARGET_SHIP] = to_num(df[TARGET_SHIP])\n",
    "\n",
    "# Applying baseline filters to keep realistic records for evaluation\n",
    "df = df.dropna(subset=[\"Order Qty\", \"Pcs\", TARGET_CUT, TARGET_SHIP])\n",
    "df = df[(df[\"Order Qty\"] >= 500) & (df[TARGET_CUT] >= 500)]\n",
    "\n",
    "# Cleaning categorical columns into standardized text format\n",
    "for c in INPUT_COLS:\n",
    "    if c not in [\"Order Qty\", \"Pcs\"]:\n",
    "        df[c] = df[c].astype(str).fillna(\"Unknown\").str.strip()\n",
    "\n",
    "# Applying rare-category bucketing to align unseen values with training logic\n",
    "for c in RARE_COLS:\n",
    "    df[c] = df[c].where(df[c].isin(meta[\"allowed_values\"][c]), OTHER_LABEL)\n",
    "\n",
    "# =========================\n",
    "# Building final model input features (including historical behavior features)\n",
    "# =========================\n",
    "X = df[INPUT_COLS].copy()\n",
    "\n",
    "hist_q, hist_t = [], []\n",
    "for _, r in df.iterrows():\n",
    "    row_dict = {k: r[k] for k in lookups[\"key_cols\"]}\n",
    "    beh = lookup_behavior(row_dict)\n",
    "    hist_q.append(beh[\"Quality_Issue_Ratio\"])\n",
    "    hist_t.append(beh[\"Net_Transfer_Ratio\"])\n",
    "\n",
    "X[\"Hist_Quality_Issue_Ratio\"] = hist_q\n",
    "X[\"Hist_Net_Transfer_Ratio\"] = hist_t\n",
    "\n",
    "# =========================\n",
    "# Running the 2-stage predictions (Cut first, then Ship)\n",
    "# =========================\n",
    "order_qty = df[\"Order Qty\"].values\n",
    "\n",
    "# Predicting Cut Ratio and converting it into Cut Qty\n",
    "cut_ratio_pred = np.clip(cut_model.predict(X), 0, 2)\n",
    "cut_qty_pred = order_qty * cut_ratio_pred\n",
    "\n",
    "# Predicting Loss Ratio using Cut Ratio as an input, then converting into Ship Qty\n",
    "X2 = X.copy()\n",
    "X2[\"Cut_Ratio\"] = cut_ratio_pred\n",
    "\n",
    "loss_ratio_pred = np.clip(loss_model.predict(X2), 0, 1)\n",
    "ship_qty_pred = cut_qty_pred * (1 - loss_ratio_pred)\n",
    "\n",
    "# =========================\n",
    "# Evaluating performance using industry standard metrics\n",
    "# =========================\n",
    "y_cut = df[TARGET_CUT].values\n",
    "y_ship = df[TARGET_SHIP].values\n",
    "\n",
    "print(\"MODEL PERFORMANCE (2026 UNSEEN DATA)\")\n",
    "print(\"Rows:\", len(df))\n",
    "\n",
    "print_metrics(\"CUT QTY\", y_cut, cut_qty_pred)\n",
    "print_metrics(\"SHIP QTY\", y_ship, ship_qty_pred)\n",
    "\n",
    "# =========================\n",
    "# Creating a quick preview output with prediction columns and error percentages\n",
    "# =========================\n",
    "out = df.copy()\n",
    "out[\"Pred_CutQty\"] = np.round(cut_qty_pred, 0)\n",
    "out[\"Pred_ShipQty\"] = np.round(ship_qty_pred, 0)\n",
    "\n",
    "eps = 1e-9\n",
    "out[\"Cut_Error_%\"] = (np.abs(out[\"Pred_CutQty\"] - out[TARGET_CUT]) / np.maximum(np.abs(out[TARGET_CUT]), eps)) * 100\n",
    "out[\"Ship_Error_%\"] = (np.abs(out[\"Pred_ShipQty\"] - out[TARGET_SHIP]) / np.maximum(np.abs(out[TARGET_SHIP]), eps)) * 100\n",
    "\n",
    "print(\"\\nSample comparison (first 10 rows):\")\n",
    "print(out[[\n",
    "    \"Order Qty\", TARGET_CUT, TARGET_SHIP,\n",
    "    \"Pred_CutQty\", \"Pred_ShipQty\",\n",
    "    \"Cut_Error_%\", \"Ship_Error_%\"\n",
    "]].head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f94ad10-774c-4359-bb18-9af3812af253",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
