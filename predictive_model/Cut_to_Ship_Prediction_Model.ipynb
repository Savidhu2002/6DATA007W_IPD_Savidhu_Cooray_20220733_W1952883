{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "48549dd4-8dab-45cc-a79a-9ab502d56ccb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows ready for training: 13444\n",
      "\n",
      "STAGE A (Cut_Ratio) evaluation\n",
      "MAE: 0.023619205007307537\n",
      "R2 : 0.06624491296019774\n",
      "\n",
      "STAGE B (Loss_Ratio) evaluation\n",
      "MAE: 0.022096276532869007\n",
      "R2 : 0.5292730587231462\n",
      "\n",
      "Saved Stage A model: D:\\Savidhu_OneDrive\\OneDrive - Hirdaramani Group\\Projects\\Cut to Ship Prediction Model\\model_cut_ratio.pkl\n",
      "Saved Stage B model: D:\\Savidhu_OneDrive\\OneDrive - Hirdaramani Group\\Projects\\Cut to Ship Prediction Model\\model_loss_ratio.pkl\n",
      "Saved meta: D:\\Savidhu_OneDrive\\OneDrive - Hirdaramani Group\\Projects\\Cut to Ship Prediction Model\\model_meta.pkl\n",
      "\n",
      "Streamlit app written to: D:\\Savidhu_OneDrive\\OneDrive - Hirdaramani Group\\Projects\\Cut to Ship Prediction Model\\streamlit_app.py\n",
      "If it does not open automatically: http://localhost:8501\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import subprocess\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "\n",
    "# =========================\n",
    "# CONFIG\n",
    "# =========================\n",
    "EXCEL_PATH = r\"D:\\Savidhu_OneDrive\\OneDrive - Hirdaramani Group\\Projects\\Cut to Ship Prediction Model\\Cut to Ship Report.xlsx\"\n",
    "SHEET_NAME = \"Final\"\n",
    "\n",
    "# Final UI inputs\n",
    "INPUT_COLS = [\n",
    "    \"Year\",\n",
    "    \"Calling Name\",\n",
    "    \"Div\",\n",
    "    \"Season\",\n",
    "    \"Garment item type\",\n",
    "    \"Unit\",\n",
    "    \"Operation\",\n",
    "    \"Month\",\n",
    "    \"Type\",\n",
    "    \"Operation 2\",\n",
    "    \"Pcs\",\n",
    "    \"Order Qty\",\n",
    "]\n",
    "\n",
    "TARGET_CUT = \"Cut Qty\"\n",
    "TARGET_SHIP = \"Ship Qty\"\n",
    "\n",
    "# Rare category handling\n",
    "RARE_COLS = [\"Div\", \"Season\", \"Calling Name\", \"Garment item type\"]\n",
    "MIN_COUNT = 10\n",
    "OTHER_LABEL = \"OTHER\"\n",
    "\n",
    "# Reason columns (damage + adjustments)\n",
    "# This is based on your file structure description. If any are missing, code will ignore them safely.\n",
    "REASON_COLS = [\n",
    "    \"Fabric Damage\",\n",
    "    \"Colour Shading\",\n",
    "    \"Finishing Damage\",\n",
    "    \"Shade Band\",\n",
    "    \"Pilot\",\n",
    "    \"Wash Reference Sample\",\n",
    "    \"Cut panel rejection qty\",\n",
    "    \"Sewing Reject Qty\",\n",
    "    \"EMB / Printing  Damages\",\n",
    "    \"Washing Damages\",\n",
    "    \"Sample qty\",\n",
    "    \"Shortage qty\",\n",
    "    \"Unreconciled qty -panel form\",\n",
    "    \"Unreconciled qty -GMT form\",\n",
    "    \"Second Quality\",\n",
    "    \"Good garments\",\n",
    "    \"PO Mix\",\n",
    "    \"Transfer to other SOD\",\n",
    "    \"Transfer from other SOD\",\n",
    "]\n",
    "\n",
    "OUTPUT_DIR = os.path.dirname(EXCEL_PATH)\n",
    "MODEL_CUT_PATH = os.path.join(OUTPUT_DIR, \"model_cut_ratio.pkl\")\n",
    "MODEL_LOSS_PATH = os.path.join(OUTPUT_DIR, \"model_loss_ratio.pkl\")\n",
    "META_PATH = os.path.join(OUTPUT_DIR, \"model_meta.pkl\")\n",
    "APP_PATH = os.path.join(OUTPUT_DIR, \"streamlit_app.py\")\n",
    "\n",
    "# =========================\n",
    "# HELPERS\n",
    "# =========================\n",
    "def to_num(series):\n",
    "    s = (\n",
    "        series.astype(str)\n",
    "        .str.replace(\",\", \"\", regex=False)\n",
    "        .str.replace(\"%\", \"\", regex=False)\n",
    "        .str.strip()\n",
    "    )\n",
    "    return pd.to_numeric(s, errors=\"coerce\")\n",
    "\n",
    "def group_rare(df, col, min_count=10, other_label=\"OTHER\"):\n",
    "    vc = df[col].value_counts(dropna=False)\n",
    "    rare_vals = vc[vc < min_count].index\n",
    "    df[col] = df[col].replace(rare_vals, other_label)\n",
    "    return df\n",
    "\n",
    "def safe_sum(df, cols):\n",
    "    present = [c for c in cols if c in df.columns]\n",
    "    if not present:\n",
    "        return pd.Series(0.0, index=df.index)\n",
    "    return df[present].sum(axis=1)\n",
    "\n",
    "# Build lookup tables for historical behavior features with fallback levels\n",
    "def build_lookup_tables(df, key_cols, feat_cols):\n",
    "    # Full key\n",
    "    full = df.groupby(key_cols, dropna=False)[feat_cols].mean().reset_index()\n",
    "\n",
    "    # Backoff 1: remove Calling Name (more general)\n",
    "    backoff1_keys = [c for c in key_cols if c != \"Calling Name\"]\n",
    "    backoff1 = df.groupby(backoff1_keys, dropna=False)[feat_cols].mean().reset_index()\n",
    "\n",
    "    # Backoff 2: remove Calling Name + Season (even more general)\n",
    "    backoff2_keys = [c for c in backoff1_keys if c != \"Season\"]\n",
    "    backoff2 = df.groupby(backoff2_keys, dropna=False)[feat_cols].mean().reset_index()\n",
    "\n",
    "    # Global mean\n",
    "    global_mean = df[feat_cols].mean().to_dict()\n",
    "\n",
    "    return {\n",
    "        \"key_cols\": key_cols,\n",
    "        \"backoff1_keys\": backoff1_keys,\n",
    "        \"backoff2_keys\": backoff2_keys,\n",
    "        \"full\": full,\n",
    "        \"backoff1\": backoff1,\n",
    "        \"backoff2\": backoff2,\n",
    "        \"global_mean\": global_mean,\n",
    "        \"feat_cols\": feat_cols\n",
    "    }\n",
    "\n",
    "def lookup_behavior(row_dict, lookups):\n",
    "    feat_cols = lookups[\"feat_cols\"]\n",
    "    global_mean = lookups[\"global_mean\"]\n",
    "\n",
    "    # Try full\n",
    "    full_keys = lookups[\"key_cols\"]\n",
    "    full_df = lookups[\"full\"]\n",
    "    mask = np.ones(len(full_df), dtype=bool)\n",
    "    for k in full_keys:\n",
    "        mask &= (full_df[k].astype(str).values == str(row_dict[k]))\n",
    "    match = full_df.loc[mask]\n",
    "    if len(match) > 0:\n",
    "        return {f: float(match.iloc[0][f]) for f in feat_cols}\n",
    "\n",
    "    # Try backoff1\n",
    "    b1_keys = lookups[\"backoff1_keys\"]\n",
    "    b1_df = lookups[\"backoff1\"]\n",
    "    mask = np.ones(len(b1_df), dtype=bool)\n",
    "    for k in b1_keys:\n",
    "        mask &= (b1_df[k].astype(str).values == str(row_dict[k]))\n",
    "    match = b1_df.loc[mask]\n",
    "    if len(match) > 0:\n",
    "        return {f: float(match.iloc[0][f]) for f in feat_cols}\n",
    "\n",
    "    # Try backoff2\n",
    "    b2_keys = lookups[\"backoff2_keys\"]\n",
    "    b2_df = lookups[\"backoff2\"]\n",
    "    mask = np.ones(len(b2_df), dtype=bool)\n",
    "    for k in b2_keys:\n",
    "        mask &= (b2_df[k].astype(str).values == str(row_dict[k]))\n",
    "    match = b2_df.loc[mask]\n",
    "    if len(match) > 0:\n",
    "        return {f: float(match.iloc[0][f]) for f in feat_cols}\n",
    "\n",
    "    # Fall back to global\n",
    "    return {f: float(global_mean.get(f, 0.0)) for f in feat_cols}\n",
    "\n",
    "# =========================\n",
    "# LOAD DATA\n",
    "# =========================\n",
    "df = pd.read_excel(EXCEL_PATH, sheet_name=SHEET_NAME)\n",
    "df.columns = [str(c).strip() for c in df.columns]\n",
    "\n",
    "required = INPUT_COLS + [TARGET_CUT, TARGET_SHIP]\n",
    "missing = [c for c in required if c not in df.columns]\n",
    "if missing:\n",
    "    raise ValueError(f\"Missing columns in sheet '{SHEET_NAME}': {missing}\")\n",
    "\n",
    "# Keep relevant columns plus any reason cols that exist\n",
    "present_reason_cols = [c for c in REASON_COLS if c in df.columns]\n",
    "df = df[INPUT_COLS + [TARGET_CUT, TARGET_SHIP] + present_reason_cols].copy()\n",
    "\n",
    "# Numeric conversions\n",
    "df[\"Order Qty\"] = to_num(df[\"Order Qty\"])\n",
    "df[\"Pcs\"] = to_num(df[\"Pcs\"])\n",
    "df[TARGET_CUT] = to_num(df[TARGET_CUT])\n",
    "df[TARGET_SHIP] = to_num(df[TARGET_SHIP])\n",
    "\n",
    "for c in present_reason_cols:\n",
    "    df[c] = to_num(df[c]).fillna(0)\n",
    "\n",
    "# Basic cleaning\n",
    "df = df.dropna(subset=[\"Order Qty\", \"Pcs\", TARGET_CUT, TARGET_SHIP])\n",
    "df = df[(df[\"Order Qty\"] > 0) & (df[\"Order Qty\"] < 1_000_000)]\n",
    "df = df[(df[\"Pcs\"] > 0) & (df[\"Pcs\"] < 1_000_000)]\n",
    "df = df[(df[TARGET_CUT] >= 0) & (df[TARGET_CUT] < 1_000_000)]\n",
    "df = df[(df[TARGET_SHIP] >= 0) & (df[TARGET_SHIP] < 1_000_000)]\n",
    "\n",
    "# Force categorical strings\n",
    "for c in INPUT_COLS:\n",
    "    if c not in [\"Order Qty\", \"Pcs\"]:\n",
    "        df[c] = df[c].astype(str).fillna(\"Unknown\").str.strip()\n",
    "\n",
    "# Rare bucketing for key categorical columns\n",
    "for c in RARE_COLS:\n",
    "    df = group_rare(df, c, min_count=MIN_COUNT, other_label=OTHER_LABEL)\n",
    "\n",
    "# =========================\n",
    "# ENGINEER TARGETS (2-stage)\n",
    "# =========================\n",
    "eps = 1e-9\n",
    "df[\"Cut_Ratio\"] = df[TARGET_CUT] / (df[\"Order Qty\"] + eps)\n",
    "df[\"Loss_Ratio\"] = (df[TARGET_CUT] - df[TARGET_SHIP]) / (df[TARGET_CUT] + eps)\n",
    "\n",
    "# Clip to sane ranges (helps stability)\n",
    "df = df[(df[\"Cut_Ratio\"] > 0) & (df[\"Cut_Ratio\"] < 2)]\n",
    "df = df[(df[\"Loss_Ratio\"] >= 0) & (df[\"Loss_Ratio\"] < 1)]\n",
    "\n",
    "# =========================\n",
    "# ENGINEER HISTORICAL BEHAVIOR FEATURES FROM REASONS\n",
    "# =========================\n",
    "# Build aggregated damage features as ratios to cut\n",
    "transfer_to = df[\"Transfer to other SOD\"] if \"Transfer to other SOD\" in df.columns else 0.0\n",
    "transfer_from = df[\"Transfer from other SOD\"] if \"Transfer from other SOD\" in df.columns else 0.0\n",
    "\n",
    "# \"Quality loss\" reasons (excluding transfers and \"Good garments\" etc. because those can be confusing)\n",
    "quality_reason_candidates = [\n",
    "    \"Fabric Damage\",\n",
    "    \"Colour Shading\",\n",
    "    \"Finishing Damage\",\n",
    "    \"Shade Band\",\n",
    "    \"Pilot\",\n",
    "    \"Wash Reference Sample\",\n",
    "    \"Cut panel rejection qty\",\n",
    "    \"Sewing Reject Qty\",\n",
    "    \"EMB / Printing  Damages\",\n",
    "    \"Washing Damages\",\n",
    "    \"Sample qty\",\n",
    "    \"Shortage qty\",\n",
    "    \"Unreconciled qty -panel form\",\n",
    "    \"Unreconciled qty -GMT form\",\n",
    "    \"Second Quality\",\n",
    "    \"PO Mix\",\n",
    "]\n",
    "quality_cols = [c for c in quality_reason_candidates if c in df.columns]\n",
    "\n",
    "df[\"Quality_Issue_Qty\"] = safe_sum(df, quality_cols)\n",
    "df[\"Net_Transfer_Qty\"] = (transfer_from - transfer_to) if isinstance(transfer_from, pd.Series) else 0.0\n",
    "\n",
    "df[\"Quality_Issue_Ratio\"] = df[\"Quality_Issue_Qty\"] / (df[TARGET_CUT] + eps)\n",
    "df[\"Net_Transfer_Ratio\"] = df[\"Net_Transfer_Qty\"] / (df[TARGET_CUT] + eps)\n",
    "\n",
    "# Clip ratios\n",
    "df[\"Quality_Issue_Ratio\"] = df[\"Quality_Issue_Ratio\"].clip(0, 1)\n",
    "df[\"Net_Transfer_Ratio\"] = df[\"Net_Transfer_Ratio\"].clip(-1, 1)\n",
    "\n",
    "# =========================\n",
    "# BUILD LOOKUPS (historical behavior features, no user inputs)\n",
    "# =========================\n",
    "KEY_COLS = [\n",
    "    \"Year\",\n",
    "    \"Calling Name\",\n",
    "    \"Div\",\n",
    "    \"Season\",\n",
    "    \"Garment item type\",\n",
    "    \"Unit\",\n",
    "    \"Operation\",\n",
    "    \"Month\",\n",
    "    \"Type\",\n",
    "    \"Operation 2\",\n",
    "]\n",
    "\n",
    "BEHAVIOR_FEATURES = [\"Quality_Issue_Ratio\", \"Net_Transfer_Ratio\"]\n",
    "\n",
    "lookups = build_lookup_tables(df, KEY_COLS, BEHAVIOR_FEATURES)\n",
    "\n",
    "# Apply lookups to each row to create \"expected behavior\" features used by the model\n",
    "# (This makes the model learn loss tendencies for similar orders)\n",
    "def add_behavior_features(df, lookups):\n",
    "    out_quality = []\n",
    "    out_transfer = []\n",
    "    for _, r in df.iterrows():\n",
    "        row_dict = {k: r[k] for k in lookups[\"key_cols\"]}\n",
    "        feats = lookup_behavior(row_dict, lookups)\n",
    "        out_quality.append(feats[\"Quality_Issue_Ratio\"])\n",
    "        out_transfer.append(feats[\"Net_Transfer_Ratio\"])\n",
    "    df[\"Hist_Quality_Issue_Ratio\"] = out_quality\n",
    "    df[\"Hist_Net_Transfer_Ratio\"] = out_transfer\n",
    "    return df\n",
    "\n",
    "df = add_behavior_features(df, lookups)\n",
    "\n",
    "print(\"Rows ready for training:\", len(df))\n",
    "\n",
    "# =========================\n",
    "# STAGE A MODEL: predict Cut_Ratio\n",
    "# =========================\n",
    "X_base = df[INPUT_COLS].copy()\n",
    "X_base[\"Hist_Quality_Issue_Ratio\"] = df[\"Hist_Quality_Issue_Ratio\"].astype(float)\n",
    "X_base[\"Hist_Net_Transfer_Ratio\"] = df[\"Hist_Net_Transfer_Ratio\"].astype(float)\n",
    "\n",
    "y_cut = df[\"Cut_Ratio\"].astype(float)\n",
    "\n",
    "cat_cols = [c for c in INPUT_COLS if c not in [\"Order Qty\", \"Pcs\"]]\n",
    "num_cols = [\"Order Qty\", \"Pcs\", \"Hist_Quality_Issue_Ratio\", \"Hist_Net_Transfer_Ratio\"]\n",
    "\n",
    "preprocess = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), cat_cols),\n",
    "        (\"num\", \"passthrough\", num_cols),\n",
    "    ]\n",
    ")\n",
    "\n",
    "cut_model = Pipeline(steps=[\n",
    "    (\"prep\", preprocess),\n",
    "    (\"model\", RandomForestRegressor(\n",
    "        n_estimators=600,\n",
    "        max_depth=22,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    ))\n",
    "])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_base, y_cut, test_size=0.2, random_state=42)\n",
    "cut_model.fit(X_train, y_train)\n",
    "\n",
    "pred_cut = cut_model.predict(X_test)\n",
    "print(\"\\nSTAGE A (Cut_Ratio) evaluation\")\n",
    "print(\"MAE:\", mean_absolute_error(y_test, pred_cut))\n",
    "print(\"R2 :\", r2_score(y_test, pred_cut))\n",
    "\n",
    "# =========================\n",
    "# STAGE B MODEL: predict Loss_Ratio\n",
    "# Include actual Cut_Ratio as an input feature (represents planned cutting strategy)\n",
    "# At runtime we will use predicted Cut_Ratio.\n",
    "# =========================\n",
    "X_loss = X_base.copy()\n",
    "X_loss[\"Cut_Ratio\"] = df[\"Cut_Ratio\"].astype(float)\n",
    "y_loss = df[\"Loss_Ratio\"].astype(float)\n",
    "\n",
    "num_cols_loss = num_cols + [\"Cut_Ratio\"]\n",
    "\n",
    "preprocess_loss = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), cat_cols),\n",
    "        (\"num\", \"passthrough\", num_cols_loss),\n",
    "    ]\n",
    ")\n",
    "\n",
    "loss_model = Pipeline(steps=[\n",
    "    (\"prep\", preprocess_loss),\n",
    "    (\"model\", RandomForestRegressor(\n",
    "        n_estimators=600,\n",
    "        max_depth=22,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    ))\n",
    "])\n",
    "\n",
    "X_train2, X_test2, y_train2, y_test2 = train_test_split(X_loss, y_loss, test_size=0.2, random_state=42)\n",
    "loss_model.fit(X_train2, y_train2)\n",
    "\n",
    "pred_loss = loss_model.predict(X_test2)\n",
    "print(\"\\nSTAGE B (Loss_Ratio) evaluation\")\n",
    "print(\"MAE:\", mean_absolute_error(y_test2, pred_loss))\n",
    "print(\"R2 :\", r2_score(y_test2, pred_loss))\n",
    "\n",
    "# =========================\n",
    "# SAVE MODELS + META (including lookups + allowed values)\n",
    "# =========================\n",
    "joblib.dump(cut_model, MODEL_CUT_PATH)\n",
    "joblib.dump(loss_model, MODEL_LOSS_PATH)\n",
    "\n",
    "allowed_values = {}\n",
    "for c in INPUT_COLS:\n",
    "    if c not in [\"Order Qty\", \"Pcs\"]:\n",
    "        allowed_values[c] = sorted(df[c].astype(str).unique().tolist())\n",
    "\n",
    "meta = {\n",
    "    \"excel_path\": EXCEL_PATH,\n",
    "    \"sheet_name\": SHEET_NAME,\n",
    "    \"input_cols\": INPUT_COLS,\n",
    "    \"key_cols\": KEY_COLS,\n",
    "    \"rare_cols\": RARE_COLS,\n",
    "    \"min_count\": MIN_COUNT,\n",
    "    \"other_label\": OTHER_LABEL,\n",
    "    \"allowed_values\": allowed_values,\n",
    "    \"behavior_features\": BEHAVIOR_FEATURES,\n",
    "    \"lookups\": {\n",
    "        \"key_cols\": lookups[\"key_cols\"],\n",
    "        \"backoff1_keys\": lookups[\"backoff1_keys\"],\n",
    "        \"backoff2_keys\": lookups[\"backoff2_keys\"],\n",
    "        \"full\": lookups[\"full\"],\n",
    "        \"backoff1\": lookups[\"backoff1\"],\n",
    "        \"backoff2\": lookups[\"backoff2\"],\n",
    "        \"global_mean\": lookups[\"global_mean\"],\n",
    "        \"feat_cols\": lookups[\"feat_cols\"]\n",
    "    }\n",
    "}\n",
    "joblib.dump(meta, META_PATH)\n",
    "\n",
    "print(\"\\nSaved Stage A model:\", MODEL_CUT_PATH)\n",
    "print(\"Saved Stage B model:\", MODEL_LOSS_PATH)\n",
    "print(\"Saved meta:\", META_PATH)\n",
    "\n",
    "# =========================\n",
    "# WRITE STREAMLIT APP (2-stage, no reason inputs)\n",
    "# =========================\n",
    "app_code = f\"\"\"\n",
    "import streamlit as st\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "\n",
    "MODEL_CUT_PATH = r\"{MODEL_CUT_PATH}\"\n",
    "MODEL_LOSS_PATH = r\"{MODEL_LOSS_PATH}\"\n",
    "META_PATH = r\"{META_PATH}\"\n",
    "\n",
    "cut_model = joblib.load(MODEL_CUT_PATH)\n",
    "loss_model = joblib.load(MODEL_LOSS_PATH)\n",
    "meta = joblib.load(META_PATH)\n",
    "\n",
    "INPUT_COLS = meta[\"input_cols\"]\n",
    "ALLOWED = meta[\"allowed_values\"]\n",
    "RARE_COLS = set(meta[\"rare_cols\"])\n",
    "OTHER_LABEL = meta[\"other_label\"]\n",
    "\n",
    "lookups = meta[\"lookups\"]\n",
    "feat_cols = lookups[\"feat_cols\"]\n",
    "\n",
    "full_df = pd.DataFrame(lookups[\"full\"])\n",
    "b1_df = pd.DataFrame(lookups[\"backoff1\"])\n",
    "b2_df = pd.DataFrame(lookups[\"backoff2\"])\n",
    "global_mean = lookups[\"global_mean\"]\n",
    "\n",
    "def lookup_behavior(row_dict):\n",
    "    # Try full\n",
    "    mask = np.ones(len(full_df), dtype=bool)\n",
    "    for k in lookups[\"key_cols\"]:\n",
    "        mask &= (full_df[k].astype(str).values == str(row_dict[k]))\n",
    "    m = full_df.loc[mask]\n",
    "    if len(m) > 0:\n",
    "        return {{f: float(m.iloc[0][f]) for f in feat_cols}}\n",
    "\n",
    "    # Backoff1\n",
    "    mask = np.ones(len(b1_df), dtype=bool)\n",
    "    for k in lookups[\"backoff1_keys\"]:\n",
    "        mask &= (b1_df[k].astype(str).values == str(row_dict[k]))\n",
    "    m = b1_df.loc[mask]\n",
    "    if len(m) > 0:\n",
    "        return {{f: float(m.iloc[0][f]) for f in feat_cols}}\n",
    "\n",
    "    # Backoff2\n",
    "    mask = np.ones(len(b2_df), dtype=bool)\n",
    "    for k in lookups[\"backoff2_keys\"]:\n",
    "        mask &= (b2_df[k].astype(str).values == str(row_dict[k]))\n",
    "    m = b2_df.loc[mask]\n",
    "    if len(m) > 0:\n",
    "        return {{f: float(m.iloc[0][f]) for f in feat_cols}}\n",
    "\n",
    "    # Global\n",
    "    return {{f: float(global_mean.get(f, 0.0)) for f in feat_cols}}\n",
    "\n",
    "st.set_page_config(page_title=\"Cut to Ship Prediction\", layout=\"wide\")\n",
    "st.title(\"Cut to Ship Prediction (2-Stage Model)\")\n",
    "\n",
    "st.write(\n",
    "    \"Stage A predicts the cutting plan (Cut Qty based on Cut Ratio). \"\n",
    "    \"Stage B predicts execution loss (Ship Qty based on Loss Ratio). \"\n",
    "    \"The reasons data is used as historical behavior patterns, so users do not need to enter it.\"\n",
    ")\n",
    "\n",
    "st.subheader(\"Enter Order Details\")\n",
    "cols = st.columns(3)\n",
    "inputs = {{}}\n",
    "\n",
    "for i, col in enumerate(INPUT_COLS):\n",
    "    with cols[i % 3]:\n",
    "        if col in [\"Order Qty\", \"Pcs\"]:\n",
    "            inputs[col] = st.number_input(col, min_value=1, step=1)\n",
    "        else:\n",
    "            options = ALLOWED.get(col, [])\n",
    "            if options:\n",
    "                inputs[col] = st.selectbox(col, options)\n",
    "            else:\n",
    "                inputs[col] = st.text_input(col, value=\"Unknown\")\n",
    "\n",
    "# Force strings for categoricals\n",
    "for c in INPUT_COLS:\n",
    "    if c not in [\"Order Qty\", \"Pcs\"]:\n",
    "        inputs[c] = str(inputs[c]).strip()\n",
    "\n",
    "# Rare bucketing at input time\n",
    "for c in RARE_COLS:\n",
    "    if inputs.get(c, OTHER_LABEL) not in ALLOWED.get(c, []):\n",
    "        inputs[c] = OTHER_LABEL\n",
    "\n",
    "# Lookup historical behavior features\n",
    "row_dict = {{k: inputs[k] for k in lookups[\"key_cols\"]}}\n",
    "beh = lookup_behavior(row_dict)\n",
    "\n",
    "X = pd.DataFrame([[inputs[c] for c in INPUT_COLS]], columns=INPUT_COLS)\n",
    "X[\"Hist_Quality_Issue_Ratio\"] = beh.get(\"Quality_Issue_Ratio\", 0.0)\n",
    "X[\"Hist_Net_Transfer_Ratio\"] = beh.get(\"Net_Transfer_Ratio\", 0.0)\n",
    "\n",
    "if st.button(\"Predict\"):\n",
    "    order_qty = float(inputs[\"Order Qty\"])\n",
    "    eps = 1e-9\n",
    "\n",
    "    # Stage A\n",
    "    cut_ratio_pred = float(cut_model.predict(X)[0])\n",
    "    cut_ratio_pred = max(0.0, min(2.0, cut_ratio_pred))\n",
    "    cut_qty_pred = order_qty * cut_ratio_pred\n",
    "\n",
    "    # Stage B (add cut ratio as a feature)\n",
    "    X2 = X.copy()\n",
    "    X2[\"Cut_Ratio\"] = cut_ratio_pred\n",
    "    loss_ratio_pred = float(loss_model.predict(X2)[0])\n",
    "    loss_ratio_pred = max(0.0, min(1.0, loss_ratio_pred))\n",
    "\n",
    "    ship_qty_pred = cut_qty_pred * (1 - loss_ratio_pred)\n",
    "\n",
    "    # Ratios\n",
    "    cut_ship = ship_qty_pred / (cut_qty_pred + eps)\n",
    "    order_ship = ship_qty_pred / (order_qty + eps)\n",
    "    order_cut = cut_qty_pred / (order_qty + eps)\n",
    "\n",
    "    # Simple risk logic\n",
    "    if cut_ship < 0.95 or order_ship < 0.95:\n",
    "        risk = \"HIGH RISK\"\n",
    "        color = \"red\"\n",
    "    elif cut_ship < 0.98 or order_ship < 0.98:\n",
    "        risk = \"MEDIUM RISK\"\n",
    "        color = \"orange\"\n",
    "    else:\n",
    "        risk = \"LOW RISK\"\n",
    "        color = \"green\"\n",
    "\n",
    "    st.subheader(\"Prediction Results\")\n",
    "\n",
    "    c1, c2, c3 = st.columns(3)\n",
    "    c1.metric(\"Predicted Cut Qty\", f\"{{int(round(cut_qty_pred)):,}}\")\n",
    "    c2.metric(\"Predicted Ship Qty\", f\"{{int(round(ship_qty_pred)):,}}\")\n",
    "    c3.metric(\"Order Qty\", f\"{{int(order_qty):,}}\")\n",
    "\n",
    "    c4, c5, c6 = st.columns(3)\n",
    "    c4.metric(\"Cut / Ship\", round(float(cut_ship), 3))\n",
    "    c5.metric(\"Order / Ship\", round(float(order_ship), 3))\n",
    "    c6.metric(\"Order / Cut\", round(float(order_cut), 3))\n",
    "\n",
    "    st.markdown(f\"<h3 style='color:{{color}}'>Overall Risk: {{risk}}</h3>\", unsafe_allow_html=True)\n",
    "\n",
    "    with st.expander(\"Model details used (historical behavior features)\"):\n",
    "        st.write({{\n",
    "            \"Hist_Quality_Issue_Ratio\": float(X[\"Hist_Quality_Issue_Ratio\"].iloc[0]),\n",
    "            \"Hist_Net_Transfer_Ratio\": float(X[\"Hist_Net_Transfer_Ratio\"].iloc[0]),\n",
    "            \"Pred_Cut_Ratio\": float(cut_ratio_pred),\n",
    "            \"Pred_Loss_Ratio\": float(loss_ratio_pred),\n",
    "        }})\n",
    "\"\"\"\n",
    "\n",
    "with open(APP_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(app_code)\n",
    "\n",
    "print(\"\\nStreamlit app written to:\", APP_PATH)\n",
    "\n",
    "# =========================\n",
    "# LAUNCH STREAMLIT FROM JUPYTER\n",
    "# =========================\n",
    "time.sleep(1)\n",
    "subprocess.Popen([sys.executable, \"-m\", \"streamlit\", \"run\", APP_PATH], cwd=OUTPUT_DIR)\n",
    "print(\"If it does not open automatically: http://localhost:8501\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "38da7524-3dc1-42be-b621-80fa36e49441",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL PERFORMANCE (2026 UNSEEN DATA)\n",
      "Rows: 112\n",
      "\n",
      "CUT QTY\n",
      "MAE: 68.78\n",
      "MAPE: 1.39%\n",
      "Accuracy (100 - MAPE): 98.61%\n",
      "R²: 0.9993\n",
      "Within ±5%: 98.21%\n",
      "Within ±10%: 99.11%\n",
      "\n",
      "SHIP QTY\n",
      "MAE: 173.30\n",
      "MAPE: 2.92%\n",
      "Accuracy (100 - MAPE): 97.08%\n",
      "R²: 0.9973\n",
      "Within ±5%: 87.50%\n",
      "Within ±10%: 99.11%\n",
      "\n",
      "Sample comparison (first 10 rows):\n",
      "   Order Qty  Cut Qty  Ship Qty  Pred_CutQty  Pred_ShipQty  Cut_Error_%  \\\n",
      "0       2404   2427.0      2298       2435.0        2337.0     0.329625   \n",
      "1      25788  26098.0     26002      26214.0       25026.0     0.444479   \n",
      "2       4646   4801.0      4699       4761.0        4593.0     0.833160   \n",
      "3      11479  11822.0     11668      12196.0       11484.0     3.163593   \n",
      "4        889    920.0       897        903.0         871.0     1.847826   \n",
      "5       2719   2850.0      2829       2784.0        2648.0     2.315789   \n",
      "6       4797   5009.0      4994       4916.0        4731.0     1.856658   \n",
      "7       4751   4983.0      4974       4869.0        4689.0     2.287778   \n",
      "8       4740   4902.0      4958       4849.0        4650.0     1.081191   \n",
      "9       2795   2857.0      2782       2833.0        2750.0     0.840042   \n",
      "\n",
      "   Ship_Error_%  \n",
      "0      1.697128  \n",
      "1      3.753557  \n",
      "2      2.255799  \n",
      "3      1.576963  \n",
      "4      2.898551  \n",
      "5      6.398021  \n",
      "6      5.266320  \n",
      "7      5.729795  \n",
      "8      6.212182  \n",
      "9      1.150252  \n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------\n",
    "# Cut to Ship Model Evaluation (2026) - Industry Standard Metrics\n",
    "# Metrics: MAE, MAPE, Accuracy(100-MAPE), R², Within ±5%, Within ±10%\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "\n",
    "# =========================\n",
    "# CONFIG\n",
    "# =========================\n",
    "EXCEL_PATH = r\"D:\\Savidhu_OneDrive\\OneDrive - Hirdaramani Group\\Projects\\Cut to Ship Prediction Model\\Cut to Ship Report.xlsx\"\n",
    "SHEET_NAME = \"2026\"\n",
    "\n",
    "OUTPUT_DIR = os.path.dirname(EXCEL_PATH)\n",
    "MODEL_CUT_PATH = os.path.join(OUTPUT_DIR, \"model_cut_ratio.pkl\")\n",
    "MODEL_LOSS_PATH = os.path.join(OUTPUT_DIR, \"model_loss_ratio.pkl\")\n",
    "META_PATH = os.path.join(OUTPUT_DIR, \"model_meta.pkl\")\n",
    "\n",
    "TARGET_CUT = \"Cut Qty\"\n",
    "TARGET_SHIP = \"Ship Qty\"\n",
    "\n",
    "# =========================\n",
    "# HELPERS\n",
    "# =========================\n",
    "def to_num(series: pd.Series) -> pd.Series:\n",
    "    s = (\n",
    "        series.astype(str)\n",
    "        .str.replace(\",\", \"\", regex=False)\n",
    "        .str.replace(\"%\", \"\", regex=False)\n",
    "        .str.strip()\n",
    "    )\n",
    "    s = s.replace({\"()\": \"0\", \"\": np.nan, \"nan\": np.nan, \"None\": np.nan})\n",
    "    return pd.to_numeric(s, errors=\"coerce\")\n",
    "\n",
    "def mape_pct(y_true, y_pred) -> float:\n",
    "    eps = 1e-9\n",
    "    y_true = np.asarray(y_true, dtype=float)\n",
    "    y_pred = np.asarray(y_pred, dtype=float)\n",
    "    return float(np.mean(np.abs((y_true - y_pred) / np.maximum(np.abs(y_true), eps))) * 100)\n",
    "\n",
    "def accuracy_from_mape(y_true, y_pred) -> float:\n",
    "    return float(np.clip(100.0 - mape_pct(y_true, y_pred), 0.0, 100.0))\n",
    "\n",
    "def within_pct(y_true, y_pred, pct: float) -> float:\n",
    "    eps = 1e-9\n",
    "    y_true = np.asarray(y_true, dtype=float)\n",
    "    y_pred = np.asarray(y_pred, dtype=float)\n",
    "    return float(\n",
    "        np.mean(np.abs(y_true - y_pred) / np.maximum(np.abs(y_true), eps) <= pct / 100.0) * 100\n",
    "    )\n",
    "\n",
    "def print_metrics(title: str, y_true, y_pred):\n",
    "    y_true = np.asarray(y_true, dtype=float)\n",
    "    y_pred = np.asarray(y_pred, dtype=float)\n",
    "\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    mape = mape_pct(y_true, y_pred)\n",
    "    acc = accuracy_from_mape(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    w5 = within_pct(y_true, y_pred, 5)\n",
    "    w10 = within_pct(y_true, y_pred, 10)\n",
    "\n",
    "    print(f\"\\n{title}\")\n",
    "    print(f\"MAE: {mae:.2f}\")\n",
    "    print(f\"MAPE: {mape:.2f}%\")\n",
    "    print(f\"Accuracy (100 - MAPE): {acc:.2f}%\")\n",
    "    print(f\"R²: {r2:.4f}\")\n",
    "    print(f\"Within ±5%: {w5:.2f}%\")\n",
    "    print(f\"Within ±10%: {w10:.2f}%\")\n",
    "\n",
    "# =========================\n",
    "# LOAD MODELS + META\n",
    "# =========================\n",
    "cut_model = joblib.load(MODEL_CUT_PATH)\n",
    "loss_model = joblib.load(MODEL_LOSS_PATH)\n",
    "meta = joblib.load(META_PATH)\n",
    "\n",
    "INPUT_COLS = meta[\"input_cols\"]\n",
    "RARE_COLS = meta[\"rare_cols\"]\n",
    "OTHER_LABEL = meta[\"other_label\"]\n",
    "lookups = meta[\"lookups\"]\n",
    "\n",
    "full_df = pd.DataFrame(lookups[\"full\"])\n",
    "b1_df = pd.DataFrame(lookups[\"backoff1\"])\n",
    "b2_df = pd.DataFrame(lookups[\"backoff2\"])\n",
    "global_mean = lookups[\"global_mean\"]\n",
    "feat_cols = lookups[\"feat_cols\"]\n",
    "\n",
    "def lookup_behavior(row_dict):\n",
    "    for df_, keys in [\n",
    "        (full_df, lookups[\"key_cols\"]),\n",
    "        (b1_df, lookups[\"backoff1_keys\"]),\n",
    "        (b2_df, lookups[\"backoff2_keys\"]),\n",
    "    ]:\n",
    "        mask = np.ones(len(df_), dtype=bool)\n",
    "        for k in keys:\n",
    "            mask &= df_[k].astype(str).values == str(row_dict[k])\n",
    "        m = df_.loc[mask]\n",
    "        if len(m) > 0:\n",
    "            return {f: float(m.iloc[0][f]) for f in feat_cols}\n",
    "    return {f: float(global_mean.get(f, 0.0)) for f in feat_cols}\n",
    "\n",
    "# =========================\n",
    "# LOAD 2026 DATA\n",
    "# =========================\n",
    "df = pd.read_excel(EXCEL_PATH, sheet_name=SHEET_NAME)\n",
    "df.columns = [str(c).strip() for c in df.columns]\n",
    "\n",
    "required = INPUT_COLS + [TARGET_CUT, TARGET_SHIP]\n",
    "missing = [c for c in required if c not in df.columns]\n",
    "if missing:\n",
    "    raise ValueError(f\"Missing columns in 2026 sheet: {missing}\")\n",
    "\n",
    "df = df[INPUT_COLS + [TARGET_CUT, TARGET_SHIP]].copy()\n",
    "\n",
    "# Numeric cleanup\n",
    "df[\"Order Qty\"] = to_num(df[\"Order Qty\"])\n",
    "df[\"Pcs\"] = to_num(df[\"Pcs\"])\n",
    "df[TARGET_CUT] = to_num(df[TARGET_CUT])\n",
    "df[TARGET_SHIP] = to_num(df[TARGET_SHIP])\n",
    "\n",
    "# Basic filtering\n",
    "df = df.dropna(subset=[\"Order Qty\", \"Pcs\", TARGET_CUT, TARGET_SHIP])\n",
    "df = df[(df[\"Order Qty\"] >= 500) & (df[TARGET_CUT] >= 500)]\n",
    "\n",
    "# Categoricals cleanup\n",
    "for c in INPUT_COLS:\n",
    "    if c not in [\"Order Qty\", \"Pcs\"]:\n",
    "        df[c] = df[c].astype(str).fillna(\"Unknown\").str.strip()\n",
    "\n",
    "# Rare bucket handling\n",
    "for c in RARE_COLS:\n",
    "    df[c] = df[c].where(df[c].isin(meta[\"allowed_values\"][c]), OTHER_LABEL)\n",
    "\n",
    "# =========================\n",
    "# BUILD FEATURES\n",
    "# =========================\n",
    "X = df[INPUT_COLS].copy()\n",
    "\n",
    "hist_q, hist_t = [], []\n",
    "for _, r in df.iterrows():\n",
    "    row_dict = {k: r[k] for k in lookups[\"key_cols\"]}\n",
    "    beh = lookup_behavior(row_dict)\n",
    "    hist_q.append(beh[\"Quality_Issue_Ratio\"])\n",
    "    hist_t.append(beh[\"Net_Transfer_Ratio\"])\n",
    "\n",
    "X[\"Hist_Quality_Issue_Ratio\"] = hist_q\n",
    "X[\"Hist_Net_Transfer_Ratio\"] = hist_t\n",
    "\n",
    "# =========================\n",
    "# PREDICT\n",
    "# =========================\n",
    "order_qty = df[\"Order Qty\"].values\n",
    "\n",
    "cut_ratio_pred = np.clip(cut_model.predict(X), 0, 2)\n",
    "cut_qty_pred = order_qty * cut_ratio_pred\n",
    "\n",
    "X2 = X.copy()\n",
    "X2[\"Cut_Ratio\"] = cut_ratio_pred\n",
    "\n",
    "loss_ratio_pred = np.clip(loss_model.predict(X2), 0, 1)\n",
    "ship_qty_pred = cut_qty_pred * (1 - loss_ratio_pred)\n",
    "\n",
    "# =========================\n",
    "# EVALUATE (INDUSTRY STANDARD)\n",
    "# =========================\n",
    "y_cut = df[TARGET_CUT].values\n",
    "y_ship = df[TARGET_SHIP].values\n",
    "\n",
    "print(\"MODEL PERFORMANCE (2026 UNSEEN DATA)\")\n",
    "print(\"Rows:\", len(df))\n",
    "\n",
    "print_metrics(\"CUT QTY\", y_cut, cut_qty_pred)\n",
    "print_metrics(\"SHIP QTY\", y_ship, ship_qty_pred)\n",
    "\n",
    "# =========================\n",
    "# OPTIONAL: QUICK SAMPLE PREVIEW\n",
    "# =========================\n",
    "out = df.copy()\n",
    "out[\"Pred_CutQty\"] = np.round(cut_qty_pred, 0)\n",
    "out[\"Pred_ShipQty\"] = np.round(ship_qty_pred, 0)\n",
    "\n",
    "eps = 1e-9\n",
    "out[\"Cut_Error_%\"] = (np.abs(out[\"Pred_CutQty\"] - out[TARGET_CUT]) / np.maximum(np.abs(out[TARGET_CUT]), eps)) * 100\n",
    "out[\"Ship_Error_%\"] = (np.abs(out[\"Pred_ShipQty\"] - out[TARGET_SHIP]) / np.maximum(np.abs(out[TARGET_SHIP]), eps)) * 100\n",
    "\n",
    "print(\"\\nSample comparison (first 10 rows):\")\n",
    "print(out[[\n",
    "    \"Order Qty\", TARGET_CUT, TARGET_SHIP,\n",
    "    \"Pred_CutQty\", \"Pred_ShipQty\",\n",
    "    \"Cut_Error_%\", \"Ship_Error_%\"\n",
    "]].head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f94ad10-774c-4359-bb18-9af3812af253",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
